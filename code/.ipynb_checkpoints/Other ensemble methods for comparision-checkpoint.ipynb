{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boost (AdaB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías de AdaBoost y de árboles de decisión \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from imblearn.datasets import fetch_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = fetch_datasets()['abalone']\n",
    "X, y = abalone.data, abalone.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "satimage = fetch_datasets()['satimage']\n",
    "X, y = satimage.data, satimage.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-FOLD CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# kf = KFold(n_splits=2)\n",
    "\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar(X, y, clasificador=None, title=None):\n",
    "    \"\"\"\n",
    "    Esta función muestra las fronteras de decisión del clasificador ya entrenado y los ejemplos en X\n",
    "    (con el color dependiendo de y).\n",
    "    :param clasificador: Clasificador entrenado de scikit-learn\n",
    "    :param X: Matriz con los ejemplos a mostrar\n",
    "    :param y: Vector con las salidas de los ejemplos a mostrar\n",
    "    :return: Nada\n",
    "    \"\"\"\n",
    "    # Creamos los mapas de colores a utilizar\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#FFFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#FFFF00', '#0000FF'])\n",
    "\n",
    "    # Creamos la figura\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    # Primer plot a la izquierda\n",
    "    plt.subplot(111)\n",
    "\n",
    "    if clasificador is not None:\n",
    "        # Preparamos los ejemplos de entrada para poder pintar la frontera de decisión\n",
    "        # Asignamos una clase (color) a cada ejemplo de la malla en [x_min, x_max]x[y_min, y_max].\n",
    "        x_min, x_max = X[:, 0].min() * 0.9-0.05, X[:, 0].max() * 1.1\n",
    "        y_min, y_max = X[:, 1].min() * 0.9-0.05, X[:, 1].max() * 1.1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "\n",
    "        # Clasificamos los puntos\n",
    "        # <RELLENAR>\n",
    "        Z = clasificador.predict(np.hstack((xx.reshape(-1,1),yy.reshape(-1,1))))\n",
    "        # Ponemos el resultado en el formato deseado\n",
    "        # <RELLENAR>\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        # Pintamos las fronteras\n",
    "        plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.2)\n",
    "    \n",
    "    # Pintamos los puntos\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=60)\n",
    "    # Asignamos el título\n",
    "    plt.xlabel('Variable 1')\n",
    "    plt.ylabel('Variable 2')\n",
    "    if title is None:\n",
    "        plt.title(\"Ejemplos de Train\")\n",
    "    else:\n",
    "        # Establecemos el título recibido como parámetro\n",
    "        # <RELLENAR>\n",
    "        plt.title(title)    \n",
    "    \n",
    "    if clasificador is not None:\n",
    "        # Establecemos los límites\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    \n",
    "    # Mostramos la figura\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "default_classes = np.unique(y)\n",
    "print(default_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3786 instances for the majoritary class\n",
      "There are 391 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR ORIGINAL:  9.682864450127877\n",
      "Num ejemplos mayoritarios en TRAIN:  3028\n",
      "Num ejemplos minoritarios en TRAIN:  313\n",
      "Num ejemplos mayoritarios en TEST:  758\n",
      "Num ejemplos minoritarios en TEST:  78\n",
      "IR TRAIN:  9.6741214057508\n",
      "IR TEST:  9.717948717948717\n",
      "Num ejemplos mayoritarios en TRAIN:  3029\n",
      "Num ejemplos minoritarios en TRAIN:  312\n",
      "Num ejemplos mayoritarios en TEST:  757\n",
      "Num ejemplos minoritarios en TEST:  79\n",
      "IR TRAIN:  9.708333333333334\n",
      "IR TEST:  9.582278481012658\n",
      "Num ejemplos mayoritarios en TRAIN:  3029\n",
      "Num ejemplos minoritarios en TRAIN:  313\n",
      "Num ejemplos mayoritarios en TEST:  757\n",
      "Num ejemplos minoritarios en TEST:  78\n",
      "IR TRAIN:  9.677316293929712\n",
      "IR TEST:  9.705128205128204\n",
      "Num ejemplos mayoritarios en TRAIN:  3029\n",
      "Num ejemplos minoritarios en TRAIN:  313\n",
      "Num ejemplos mayoritarios en TEST:  757\n",
      "Num ejemplos minoritarios en TEST:  78\n",
      "IR TRAIN:  9.677316293929712\n",
      "IR TEST:  9.705128205128204\n",
      "Num ejemplos mayoritarios en TRAIN:  3029\n",
      "Num ejemplos minoritarios en TRAIN:  313\n",
      "Num ejemplos mayoritarios en TEST:  757\n",
      "Num ejemplos minoritarios en TEST:  78\n",
      "IR TRAIN:  9.677316293929712\n",
      "IR TEST:  9.705128205128204\n"
     ]
    }
   ],
   "source": [
    "print(\"IR ORIGINAL: \",X[y==-1].shape[0]/X[y==1].shape[0])\n",
    "# kf = KFold(n_splits=5,shuffle=True)\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"Num ejemplos mayoritarios en TRAIN: \",X_train[y_train==-1].shape[0])\n",
    "    print(\"Num ejemplos minoritarios en TRAIN: \",X_train[y_train==1].shape[0])\n",
    "    print(\"Num ejemplos mayoritarios en TEST: \",X_test[y_test==-1].shape[0])\n",
    "    print(\"Num ejemplos minoritarios en TEST: \",X_test[y_test==1].shape[0])\n",
    "    print(\"IR TRAIN: \",X_train[y_train==-1].shape[0]/X_train[y_train==1].shape[0])\n",
    "    print(\"IR TEST: \",X_test[y_test==-1].shape[0]/X_test[y_test==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2839 instances for the majoritary class\n",
      "There are 293 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "# #number of features of the dataset \n",
    "# D = X_train.shape[1]\n",
    "\n",
    "# print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "# print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el número de clasificadores base de AdaBoost (numClasificadoresBase)\n",
    "numClasificadoresBase = 10\n",
    "# Llamada al constructor del clasificador \n",
    "dtc = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators' accuracies:  [90.67, 29.78, 14.0, 68.06, 15.07, 73.33, 15.67, 84.81, 24.52, 73.92]\n",
      "Estimators' Gmeans:  [0.0, 47.5, 22.96, 0.0, 25.16, 81.65, 26.93, 0.0, 40.93, 0.0]\n",
      "Estimators' Balanced accuracies:  [50.0, 61.28, 51.42, 37.53, 53.17, 82.41, 47.17, 46.77, 58.38, 40.77]\n",
      "Estimators' accuracies:  [90.55, 46.05, 11.72, 43.54, 77.99, 17.34, 29.9, 79.67, 22.01, 87.44]\n",
      "Estimators' Gmeans:  [0.0, 63.28, 15.84, 18.95, 71.59, 29.53, 21.85, 54.65, 37.24, 0.0]\n",
      "Estimators' Balanced accuracies:  [50.0, 69.64, 51.25, 27.44, 71.97, 54.36, 23.31, 59.86, 56.94, 48.28]\n",
      "Estimators' accuracies:  [90.66, 42.51, 52.1, 33.89, 12.22, 77.37, 65.39, 44.67, 35.81, 76.41]\n",
      "Estimators' Gmeans:  [0.0, 59.93, 12.11, 51.62, 17.81, 77.17, 13.57, 61.83, 12.26, 73.08]\n",
      "Estimators' Balanced accuracies:  [50.0, 67.15, 29.88, 62.39, 51.59, 77.17, 37.21, 68.33, 21.47, 73.19]\n",
      "Estimators' accuracies:  [90.66, 11.5, 43.71, 44.79, 79.64, 22.75, 44.79, 22.04, 87.9, 36.17]\n",
      "Estimators' Gmeans:  [0.0, 15.42, 61.28, 22.27, 76.0, 38.39, 22.27, 37.35, 0.0, 54.17]\n",
      "Estimators' Balanced accuracies:  [50.0, 51.19, 68.38, 29.3, 76.12, 56.82, 29.3, 56.43, 48.48, 64.22]\n",
      "Estimators' accuracies:  [90.66, 51.26, 11.74, 20.96, 56.65, 78.92, 36.05, 42.87, 79.04, 28.38]\n",
      "Estimators' Gmeans:  [0.0, 67.31, 16.25, 35.8, 12.63, 69.8, 25.18, 60.82, 0.0, 45.83]\n",
      "Estimators' Balanced accuracies:  [50.0, 71.97, 51.32, 56.41, 32.39, 70.55, 27.36, 68.49, 43.59, 60.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "accGlobal = []\n",
    "gmeanGlobal = []\n",
    "baccGlobal = []\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase)\n",
    "    # Entrenamiento del clasificador creado\n",
    "    # <RELLENAR>\n",
    "    adaboost.fit(X_train,y_train)\n",
    "    # Lista para almacenar el accuracy de cada clasificador base\n",
    "    listaAcc = []\n",
    "    listaGmean = []\n",
    "    listaBAcc = []\n",
    "    # Por cada clasificador base\n",
    "    for i in range(len(adaboost.estimators_)):\n",
    "        # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "        # Redondear a dos decimales\n",
    "        y_pred = adaboost.estimators_[i].predict(X_test)\n",
    "        gmean = round(geometric_mean_score(y_test, y_pred)*100,2)\n",
    "        bAcc = round(balanced_accuracy_score(y_test,y_pred)*100,2)\n",
    "        acc = round(adaboost.estimators_[i].score(X_test,y_test)*100,2)\n",
    "        # Se añade a la lista de accuracies\n",
    "        listaAcc.append(acc)\n",
    "        listaGmean.append(gmean)\n",
    "        listaBAcc.append(bAcc)\n",
    "        # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "        #titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "        # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "        #mostrar(X_test,y_test,clasificador = adaboost.estimators_[i],title=titulo)\n",
    "    print(\"Estimators' accuracies: \",listaAcc)\n",
    "    print(\"Estimators' Gmeans: \",listaGmean)\n",
    "    print(\"Estimators' Balanced accuracies: \",listaBAcc)\n",
    "\n",
    "    # Se calcula el porcentaje de acierto de AdaBoost\n",
    "    acc = adaboost.score(X_test,y_test)*100\n",
    "    accGlobal.append(acc)\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    gmean = geometric_mean_score(y_test, y_pred)*100\n",
    "    gmeanGlobal.append(gmean)\n",
    "    bAcc = balanced_accuracy_score(y_test, y_pred)*100\n",
    "    baccGlobal.append(bAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.55023923444976, 90.19138755980862, 90.65868263473054, 90.2994011976048, 90.65868263473054]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[49.934036939313984, 49.80184940554822, 50.0, 49.80184940554822, 50.0]\n"
     ]
    }
   ],
   "source": [
    "print(accGlobal)\n",
    "print(gmeanGlobal)\n",
    "print(baccGlobal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar los puntos en el scatter plot se debe escoger sólo dos atriubutos\n",
    "NO SÉ HASTA QUÉ PUNTO TIENE SENTIDO Y SI ESTÁ BIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_2features = X_train[:,:2]\n",
    "# X_test_2features = X_test[:,:2]\n",
    "# # Se define el número de clasificadores base de AdaBoost (numClasificadoresBase)\n",
    "# numClasificadoresBase = 10\n",
    "# # Llamada al constructor del clasificador AdaBoost\n",
    "# dtc = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase)\n",
    "# # Entrenamiento del clasificador creado\n",
    "# # <RELLENAR>\n",
    "# adaboost.fit(X_train_2features,y_train)\n",
    "# # Lista para almacenar el accuracy de cada clasificador base\n",
    "# listaAcc = []\n",
    "# # Por cada clasificador base\n",
    "# for i in range(len(adaboost.estimators_)):\n",
    "#     # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "#     # Redondear a dos decimales\n",
    "#     acc = round(adaboost.estimators_[i].score(X_test_2features,y_test)*100,2)\n",
    "#     # Se añade a la lista de accuracies\n",
    "#     listaAcc.append(acc)\n",
    "#     # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "#     titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "#     # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "#     # <RELLENAR>\n",
    "#     mostrar(X_test_2features,y_test,clasificador = adaboost.estimators_[i],title=titulo)\n",
    "# print(listaAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.622009569378\n",
      "10.096190602031177\n",
      "50.457405771178585\n"
     ]
    }
   ],
   "source": [
    "# # Se calcula el porcentaje de acierto de AdaBoost\n",
    "# acc = adaboost.score(X_test,y_test)*100\n",
    "# print(acc)\n",
    "# y_pred = adaboost.predict(X_test)\n",
    "# gmean = geometric_mean_score(y_test, y_pred)*100\n",
    "# print(gmean)\n",
    "# bAcc = balanced_accuracy_score(y_test, y_pred)*100\n",
    "# print(bAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE-Boost (SBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "#from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class SMOTE(object):\n",
    "    \"\"\"Implementation of Synthetic Minority Over-Sampling Technique (SMOTE).\n",
    "    SMOTE performs oversampling of the minority class by picking target \n",
    "    minority class samples and their nearest minority class neighbors and \n",
    "    generating new samples that linearly combine features of each target \n",
    "    sample with features of its selected minority class neighbors [1].\n",
    "    Parameters\n",
    "    ----------\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O. Hall, and P. Kegelmeyer. \"SMOTE:\n",
    "           Synthetic Minority Over-Sampling Technique.\" Journal of Artificial\n",
    "           Intelligence Research (JAIR), 2002.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_neighbors=5, random_state=None):\n",
    "        self.k = k_neighbors\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of new synthetic samples.\n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "\n",
    "        S = np.zeros(shape=(n_samples, self.n_features))\n",
    "        # Calculate synthetic samples.\n",
    "        for i in range(n_samples):\n",
    "            j = np.random.randint(0, self.X.shape[0])\n",
    "\n",
    "            # Find the NN for each sample.\n",
    "            # Exclude the sample itself.\n",
    "            nn = self.neigh.kneighbors(self.X[j].reshape(1, -1),\n",
    "                                       return_distance=False)[:, 1:]\n",
    "            nn_index = np.random.choice(nn[0])\n",
    "\n",
    "            dif = self.X[nn_index] - self.X[j]\n",
    "            gap = np.random.random()\n",
    "\n",
    "            S[i, :] = self.X[j, :] + gap * dif[:]\n",
    "\n",
    "        return S\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_minority_samples, n_features]\n",
    "            Holds the minority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_minority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        # Learn nearest neighbors.\n",
    "        self.neigh = NearestNeighbors(n_neighbors=self.k + 1)\n",
    "        self.neigh.fit(self.X)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class SMOTEBoost(AdaBoostClassifier):\n",
    "    \"\"\"Implementation of SMOTEBoost.\n",
    "    SMOTEBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    oversampling the minority class using SMOTE on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer.\n",
    "           \"SMOTEBoost: Improving Prediction of the Minority Class in\n",
    "           Boosting.\" European Conference on Principles of Data Mining and\n",
    "           Knowledge Discovery (PKDD), 2003.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 k_neighbors=5,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.algorithm = algorithm\n",
    "        self.smote = SMOTE(k_neighbors=k_neighbors,\n",
    "                           random_state=random_state)\n",
    "\n",
    "        super(SMOTEBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing SMOTE during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        # Check parameters.\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            X_min = X[np.where(y == self.minority_target)]\n",
    "\n",
    "            # SMOTE step.\n",
    "            if len(X_min) >= self.smote.k:\n",
    "                self.smote.fit(X_min)\n",
    "                X_syn = self.smote.sample(self.n_samples)\n",
    "                y_syn = np.full(X_syn.shape[0], fill_value=self.minority_target,\n",
    "                                dtype=np.int64)\n",
    "\n",
    "                # Normalize synthetic sample weights based on current training set.\n",
    "                sample_weight_syn = np.empty(X_syn.shape[0], dtype=np.float64)\n",
    "                sample_weight_syn[:] = 1. / X.shape[0]\n",
    "\n",
    "                # Combine the original and synthetic samples.\n",
    "                X = np.vstack((X, X_syn))\n",
    "                y = np.append(y, y_syn)\n",
    "\n",
    "                # Combine the weights.\n",
    "                sample_weight = \\\n",
    "                    np.append(sample_weight, sample_weight_syn).reshape(-1, 1)\n",
    "                sample_weight = \\\n",
    "                    np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "                # X, y, sample_weight = shuffle(X, y, sample_weight,\n",
    "                #                              random_state=random_state)\n",
    "\n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination.\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class RandomUnderSampler(object):\n",
    "    \"\"\"Implementation of random undersampling (RUS).\n",
    "    Undersample the majority class(es) by randomly picking samples with or\n",
    "    without replacement.\n",
    "    Parameters\n",
    "    ----------\n",
    "    with_replacement : bool, optional (default=True)\n",
    "        Undersample with replacement.\n",
    "    return_indices : bool, optional (default=False)\n",
    "        Whether or not to return the indices of the samples randomly selected\n",
    "        from the majority class.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, with_replacement=True, return_indices=False,\n",
    "                 random_state=None):\n",
    "        self.return_indices = return_indices\n",
    "        self.with_replacement = with_replacement\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Perform undersampling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples to remove.\n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_majority_samples - n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "\n",
    "        if self.n_majority_samples <= n_samples:\n",
    "            n_samples = self.n_majority_samples\n",
    "\n",
    "        idx = np.random.choice(self.n_majority_samples,\n",
    "                               size=self.n_majority_samples - n_samples,\n",
    "                               replace=self.with_replacement)\n",
    "\n",
    "        if self.return_indices:\n",
    "            return (self.X[idx], idx)\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_majority_samples, n_features]\n",
    "            Holds the majority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_majority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class RUSBoost(AdaBoostClassifier):\n",
    "    \"\"\"Implementation of RUSBoost.\n",
    "    RUSBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    undersampling the majority class using random undersampling (with or\n",
    "    without replacement) on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    min_ratio : float (default=1.0)\n",
    "        Minimum ratio of majority to minority class samples to generate.\n",
    "    with_replacement : bool, optional (default=True)\n",
    "        Undersample with replacement.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] C. Seiffert, T. M. Khoshgoftaar, J. V. Hulse, and A. Napolitano.\n",
    "           \"RUSBoost: Improving Classification Performance when Training Data\n",
    "           is Skewed\". International Conference on Pattern Recognition\n",
    "           (ICPR), 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 min_ratio=1.0,\n",
    "                 with_replacement=True,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.min_ratio = min_ratio\n",
    "        self.algorithm = algorithm\n",
    "        self.rus = RandomUnderSampler(with_replacement=with_replacement,\n",
    "                                      return_indices=True,\n",
    "                                      random_state=random_state)\n",
    "\n",
    "        super(RUSBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing random undersampling during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        # Check parameters.\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # Random undersampling step.\n",
    "            X_maj = X[np.where(y != self.minority_target)]\n",
    "            X_min = X[np.where(y == self.minority_target)]\n",
    "            self.rus.fit(X_maj)\n",
    "\n",
    "            n_maj = X_maj.shape[0]\n",
    "            n_min = X_min.shape[0]\n",
    "            if n_maj - self.n_samples < int(n_min * self.min_ratio):\n",
    "                self.n_samples = n_maj - int(n_min * self.min_ratio)\n",
    "            X_rus, X_idx = self.rus.sample(self.n_samples)\n",
    "\n",
    "            y_rus = y[np.where(y != self.minority_target)][X_idx]\n",
    "            y_min = y[np.where(y == self.minority_target)]\n",
    "\n",
    "            sample_weight_rus = \\\n",
    "                sample_weight[np.where(y != self.minority_target)][X_idx]\n",
    "            sample_weight_min = \\\n",
    "                sample_weight[np.where(y == self.minority_target)]\n",
    "\n",
    "            # Combine the minority and majority class samples.\n",
    "            X = np.vstack((X_rus, X_min))\n",
    "            y = np.append(y_rus, y_min)\n",
    "\n",
    "            # Combine the weights.\n",
    "            sample_weight = \\\n",
    "                np.append(sample_weight_rus, sample_weight_min).reshape(-1, 1)\n",
    "            sample_weight = \\\n",
    "                np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "            # X, y, sample_weight = shuffle(X, y, sample_weight,\n",
    "            #                              random_state=random_state)\n",
    "\n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination.\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 4174 4175 4176] TEST: [   8   10   21   26   32   41   46   53   59   60   66   67   69   76\n",
      "   82   85   91   93  116  118  124  126  133  134  141  144  146  147\n",
      "  149  155  158  163  165  167  171  175  183  189  190  207  212  213\n",
      "  216  220  221  224  229  230  231  236  240  247  253  254  267  269\n",
      "  270  274  278  287  295  297  306  307  308  309  312  321  330  331\n",
      "  333  335  344  352  353  358  359  361  362  365  367  375  390  401\n",
      "  402  405  409  419  422  425  427  434  435  436  444  445  447  457\n",
      "  464  465  470  476  478  480  492  500  502  509  510  526  530  533\n",
      "  534  537  549  553  565  574  577  578  587  600  602  607  608  615\n",
      "  622  625  633  652  653  656  665  675  681  682  690  698  699  700\n",
      "  704  707  714  717  718  719  720  721  723  729  730  732  733  736\n",
      "  741  747  749  750  767  770  771  773  786  796  797  798  800  802\n",
      "  812  830  834  842  844  856  863  870  874  882  884  887  889  892\n",
      "  898  901  903  910  922  925  935  936  942  943  950  953  958  962\n",
      "  963  968  973  980  982  984  987  992  998 1001 1002 1003 1004 1005\n",
      " 1006 1008 1010 1020 1032 1043 1046 1052 1053 1059 1064 1069 1073 1093\n",
      " 1104 1110 1113 1114 1115 1117 1118 1131 1137 1142 1143 1148 1151 1154\n",
      " 1156 1158 1159 1164 1167 1175 1176 1177 1182 1184 1194 1196 1203 1206\n",
      " 1207 1218 1220 1221 1222 1225 1228 1229 1248 1257 1258 1260 1262 1275\n",
      " 1277 1279 1280 1285 1287 1298 1300 1302 1304 1316 1318 1319 1321 1333\n",
      " 1346 1347 1353 1355 1363 1368 1370 1378 1379 1390 1392 1396 1429 1439\n",
      " 1443 1445 1446 1447 1450 1452 1463 1479 1481 1482 1488 1493 1494 1506\n",
      " 1508 1513 1514 1517 1519 1530 1531 1533 1536 1537 1542 1554 1556 1560\n",
      " 1570 1577 1582 1583 1590 1593 1596 1601 1604 1609 1611 1612 1617 1619\n",
      " 1620 1623 1633 1638 1641 1648 1655 1658 1666 1667 1668 1670 1671 1676\n",
      " 1678 1682 1683 1684 1689 1693 1695 1701 1703 1704 1709 1721 1725 1727\n",
      " 1731 1741 1743 1745 1746 1750 1756 1757 1760 1762 1775 1777 1784 1793\n",
      " 1795 1797 1799 1804 1805 1809 1811 1812 1824 1825 1827 1829 1830 1832\n",
      " 1834 1844 1847 1849 1850 1860 1884 1885 1900 1909 1920 1926 1928 1932\n",
      " 1935 1946 1950 1951 1952 1955 1956 1959 1969 1970 1975 1978 1981 1983\n",
      " 1989 1990 1994 1998 1999 2008 2013 2023 2024 2027 2031 2048 2051 2060\n",
      " 2062 2064 2065 2066 2072 2074 2077 2080 2083 2088 2097 2101 2104 2118\n",
      " 2130 2144 2148 2152 2154 2158 2159 2160 2162 2183 2197 2199 2201 2230\n",
      " 2236 2240 2247 2248 2249 2250 2255 2261 2270 2275 2286 2288 2292 2295\n",
      " 2300 2303 2304 2309 2313 2319 2326 2332 2339 2347 2357 2359 2360 2392\n",
      " 2396 2398 2407 2418 2426 2427 2432 2433 2435 2440 2442 2451 2453 2455\n",
      " 2460 2461 2465 2468 2469 2471 2480 2481 2483 2485 2495 2496 2503 2505\n",
      " 2508 2509 2510 2511 2512 2515 2517 2523 2534 2536 2540 2541 2552 2577\n",
      " 2579 2583 2584 2586 2589 2590 2594 2598 2601 2605 2621 2629 2630 2631\n",
      " 2638 2639 2643 2652 2655 2656 2657 2664 2670 2674 2675 2676 2680 2687\n",
      " 2688 2691 2702 2706 2707 2710 2711 2712 2715 2717 2719 2720 2728 2738\n",
      " 2749 2762 2766 2771 2787 2791 2792 2797 2798 2800 2811 2817 2820 2821\n",
      " 2825 2826 2831 2843 2848 2865 2871 2876 2878 2885 2896 2898 2907 2911\n",
      " 2914 2921 2942 2944 2948 2951 2967 2970 2974 2975 2984 2985 2990 2991\n",
      " 2993 2994 2999 3004 3005 3006 3016 3020 3027 3035 3037 3041 3044 3052\n",
      " 3056 3060 3064 3066 3067 3072 3076 3082 3087 3099 3100 3105 3111 3130\n",
      " 3131 3134 3147 3149 3156 3158 3162 3164 3167 3169 3173 3174 3179 3182\n",
      " 3183 3184 3186 3189 3191 3194 3197 3200 3205 3206 3210 3214 3215 3217\n",
      " 3221 3222 3223 3230 3234 3235 3241 3254 3256 3257 3260 3264 3265 3266\n",
      " 3271 3288 3292 3301 3303 3308 3320 3321 3324 3325 3343 3344 3346 3350\n",
      " 3355 3356 3358 3367 3371 3373 3386 3387 3388 3403 3410 3411 3416 3417\n",
      " 3418 3420 3426 3429 3441 3445 3446 3447 3459 3462 3467 3473 3477 3482\n",
      " 3483 3486 3493 3511 3513 3530 3535 3540 3550 3553 3563 3564 3567 3576\n",
      " 3579 3585 3586 3587 3591 3598 3606 3610 3613 3625 3629 3630 3633 3637\n",
      " 3639 3656 3658 3660 3663 3674 3684 3688 3708 3715 3721 3737 3744 3746\n",
      " 3748 3749 3751 3772 3791 3792 3794 3807 3808 3811 3812 3815 3838 3849\n",
      " 3850 3851 3855 3862 3871 3880 3890 3912 3924 3942 3944 3950 3957 3973\n",
      " 3987 3994 3997 4010 4011 4024 4030 4040 4042 4043 4052 4062 4065 4068\n",
      " 4069 4085 4086 4087 4090 4103 4108 4110 4114 4116 4121 4124 4127 4129\n",
      " 4137 4144 4150 4151 4152 4158 4162 4163 4166 4170]\n",
      "Estimators' accuracies:  [90.19, 39.23, 12.92, 52.27, 75.12, 19.86, 61.36, 39.71, 79.9, 30.74]\n",
      "Estimators' Gmeans:  [0.0, 56.89, 18.57, 20.45, 73.12, 33.38, 12.86, 57.34, 0.0, 48.02]\n",
      "Estimators' Balanced accuracies:  [50.0, 65.77, 51.72, 32.24, 73.16, 55.57, 35.11, 66.03, 44.3, 61.06]\n",
      "TRAIN: [   0    1    2 ... 4173 4174 4175] TEST: [   9   20   28   34   36   37   38   40   45   49   50   56   63   65\n",
      "   68   77   84   87   89   92   94   97  103  107  110  114  115  119\n",
      "  120  122  131  137  139  150  153  154  157  162  166  169  177  178\n",
      "  196  197  202  203  209  237  238  249  255  262  263  271  275  280\n",
      "  283  288  290  294  296  298  303  324  325  328  332  339  340  341\n",
      "  346  349  356  363  370  374  379  385  395  396  410  412  421  423\n",
      "  432  440  441  463  466  469  477  481  485  493  496  497  512  514\n",
      "  516  519  539  540  544  555  556  557  562  570  575  589  591  595\n",
      "  597  601  603  616  621  623  624  630  638  639  640  641  645  662\n",
      "  664  669  672  678  679  680  683  684  687  688  689  693  694  697\n",
      "  701  702  709  711  724  725  726  727  731  734  738  740  742  743\n",
      "  752  755  758  763  774  775  781  784  792  794  803  805  806  814\n",
      "  821  825  829  835  839  847  852  857  858  859  861  864  869  881\n",
      "  888  890  891  895  896  908  913  920  923  927  929  933  937  945\n",
      "  948  954  961  966  972  983  993  996 1000 1007 1015 1016 1018 1029\n",
      " 1031 1034 1035 1036 1041 1061 1063 1068 1071 1072 1078 1082 1092 1097\n",
      " 1098 1109 1112 1125 1126 1136 1144 1150 1152 1153 1160 1165 1166 1170\n",
      " 1171 1173 1174 1179 1185 1188 1191 1195 1237 1245 1247 1249 1266 1267\n",
      " 1276 1281 1286 1289 1290 1295 1297 1309 1310 1317 1324 1325 1327 1329\n",
      " 1335 1337 1338 1341 1342 1349 1351 1359 1360 1362 1380 1394 1401 1402\n",
      " 1404 1407 1408 1409 1415 1417 1419 1430 1435 1451 1457 1461 1467 1485\n",
      " 1498 1525 1532 1538 1539 1544 1549 1557 1564 1573 1580 1584 1589 1594\n",
      " 1600 1606 1613 1625 1631 1634 1635 1637 1643 1647 1664 1672 1677 1686\n",
      " 1688 1691 1694 1698 1707 1713 1717 1724 1728 1730 1738 1742 1747 1749\n",
      " 1768 1771 1774 1778 1781 1783 1788 1794 1796 1800 1801 1802 1807 1808\n",
      " 1817 1819 1823 1826 1828 1836 1838 1848 1851 1854 1865 1866 1870 1872\n",
      " 1876 1877 1881 1882 1890 1895 1899 1902 1906 1911 1918 1921 1922 1923\n",
      " 1925 1931 1940 1942 1943 1945 1947 1958 1963 1966 1976 1977 1993 1997\n",
      " 2007 2011 2014 2026 2028 2037 2040 2045 2049 2056 2058 2070 2075 2079\n",
      " 2084 2085 2087 2094 2100 2106 2107 2110 2113 2117 2120 2121 2124 2129\n",
      " 2131 2134 2143 2147 2150 2151 2163 2165 2173 2176 2184 2203 2204 2213\n",
      " 2215 2216 2218 2220 2221 2223 2224 2227 2228 2229 2233 2234 2251 2259\n",
      " 2264 2269 2271 2274 2277 2280 2281 2282 2283 2289 2298 2312 2316 2318\n",
      " 2323 2328 2331 2333 2335 2338 2345 2348 2361 2371 2376 2377 2378 2379\n",
      " 2381 2394 2402 2405 2406 2408 2411 2412 2414 2416 2420 2425 2439 2443\n",
      " 2448 2450 2458 2462 2464 2470 2487 2489 2494 2497 2501 2502 2504 2506\n",
      " 2524 2528 2529 2542 2550 2553 2558 2560 2563 2565 2567 2570 2576 2578\n",
      " 2591 2595 2607 2611 2614 2615 2617 2627 2632 2633 2653 2667 2671 2690\n",
      " 2693 2709 2721 2724 2729 2731 2733 2734 2740 2741 2743 2745 2751 2754\n",
      " 2755 2756 2760 2765 2767 2783 2790 2793 2795 2799 2805 2810 2813 2815\n",
      " 2830 2836 2844 2859 2874 2881 2882 2884 2886 2887 2892 2899 2900 2902\n",
      " 2903 2905 2908 2910 2916 2918 2919 2922 2927 2934 2939 2940 2945 2947\n",
      " 2950 2953 2964 2969 2976 2980 2982 2983 2997 2998 3001 3003 3008 3012\n",
      " 3014 3015 3039 3043 3049 3054 3061 3062 3065 3068 3071 3074 3077 3086\n",
      " 3089 3090 3094 3098 3103 3104 3106 3119 3122 3125 3126 3128 3137 3138\n",
      " 3139 3145 3148 3157 3176 3181 3199 3203 3211 3213 3220 3228 3229 3236\n",
      " 3237 3243 3250 3251 3255 3268 3270 3274 3277 3281 3283 3285 3289 3293\n",
      " 3294 3297 3299 3306 3310 3311 3317 3326 3336 3337 3342 3351 3357 3359\n",
      " 3364 3374 3379 3392 3397 3399 3405 3414 3415 3423 3430 3434 3435 3440\n",
      " 3450 3451 3455 3457 3469 3470 3472 3487 3502 3507 3510 3515 3520 3521\n",
      " 3524 3526 3536 3541 3542 3545 3549 3551 3555 3562 3580 3584 3593 3594\n",
      " 3608 3617 3618 3619 3622 3623 3634 3636 3640 3644 3650 3654 3659 3665\n",
      " 3667 3669 3679 3683 3695 3696 3712 3713 3716 3729 3733 3736 3769 3774\n",
      " 3778 3781 3786 3788 3789 3793 3802 3803 3809 3817 3825 3830 3834 3840\n",
      " 3852 3853 3856 3864 3866 3870 3874 3877 3891 3893 3895 3902 3905 3910\n",
      " 3915 3919 3920 3929 3932 3934 3936 3939 3941 3946 3947 3948 3952 3953\n",
      " 3955 3967 3969 3971 3975 3984 3986 3992 3993 3995 3998 4003 4007 4013\n",
      " 4014 4017 4020 4027 4028 4031 4032 4036 4038 4041 4044 4045 4053 4054\n",
      " 4058 4060 4072 4074 4077 4082 4083 4100 4104 4109 4120 4126 4134 4135\n",
      " 4141 4146 4148 4149 4153 4154 4155 4165 4172 4176]\n",
      "Estimators' accuracies:  [90.79, 39.95, 11.6, 51.56, 78.35, 19.02, 39.47, 80.98, 26.56, 71.41]\n",
      "Estimators' Gmeans:  [0.0, 58.19, 16.23, 17.1, 73.89, 32.87, 20.99, 0.0, 43.71, 0.0]\n",
      "Estimators' Balanced accuracies:  [50.0, 66.93, 51.32, 30.73, 74.07, 55.4, 26.41, 44.6, 59.55, 39.33]\n",
      "TRAIN: [   1    2    3 ... 4172 4173 4176] TEST: [   0    4    7   15   17   22   23   24   27   47   57   58   61   64\n",
      "   72   74   75   78   81  101  102  104  105  113  123  125  132  136\n",
      "  138  142  143  156  159  164  168  172  173  174  176  179  182  187\n",
      "  188  194  198  200  206  210  215  235  243  250  265  268  272  273\n",
      "  276  277  279  284  292  300  305  316  319  320  334  336  347  351\n",
      "  360  366  368  369  372  378  392  394  397  398  403  404  406  424\n",
      "  430  433  438  439  442  449  451  452  458  462  467  473  474  487\n",
      "  488  491  505  508  517  523  527  529  532  543  546  547  548  552\n",
      "  561  569  580  585  592  593  596  604  606  613  617  619  629  642\n",
      "  646  647  655  659  660  666  667  670  685  692  695  710  715  737\n",
      "  751  760  765  801  808  811  816  818  819  823  828  840  848  850\n",
      "  853  862  867  871  876  877  883  885  899  907  909  916  930  932\n",
      "  939  949  955  956  957  964  977  979  989  994 1009 1011 1013 1021\n",
      " 1025 1037 1040 1057 1060 1066 1077 1080 1081 1085 1087 1095 1101 1103\n",
      " 1106 1108 1111 1120 1122 1130 1134 1139 1140 1145 1147 1155 1172 1178\n",
      " 1181 1183 1186 1189 1190 1192 1198 1202 1205 1209 1210 1212 1215 1217\n",
      " 1219 1227 1233 1236 1244 1246 1253 1255 1259 1263 1264 1268 1271 1278\n",
      " 1291 1292 1303 1311 1313 1315 1322 1323 1328 1331 1332 1345 1348 1350\n",
      " 1352 1354 1357 1358 1364 1372 1374 1376 1386 1389 1391 1397 1398 1411\n",
      " 1413 1414 1431 1437 1438 1444 1448 1456 1458 1459 1460 1469 1472 1475\n",
      " 1477 1478 1483 1484 1490 1492 1502 1505 1510 1512 1520 1522 1526 1528\n",
      " 1535 1541 1545 1546 1547 1548 1553 1558 1563 1567 1568 1569 1578 1585\n",
      " 1597 1602 1605 1608 1614 1616 1622 1628 1632 1636 1639 1644 1646 1649\n",
      " 1650 1651 1652 1661 1662 1663 1674 1675 1679 1690 1696 1697 1699 1708\n",
      " 1712 1722 1723 1732 1733 1734 1739 1740 1758 1761 1769 1772 1790 1792\n",
      " 1820 1821 1835 1840 1856 1858 1868 1874 1880 1891 1892 1896 1897 1901\n",
      " 1903 1904 1905 1907 1908 1910 1915 1916 1927 1933 1934 1938 1941 1948\n",
      " 1953 1954 1957 1964 1980 1984 1985 1988 1992 2003 2010 2012 2017 2025\n",
      " 2029 2032 2036 2043 2044 2047 2063 2069 2071 2076 2081 2089 2090 2092\n",
      " 2095 2098 2099 2105 2108 2112 2115 2116 2126 2135 2139 2145 2153 2157\n",
      " 2164 2168 2169 2170 2178 2179 2180 2182 2187 2189 2190 2191 2195 2200\n",
      " 2208 2210 2225 2226 2231 2242 2243 2244 2245 2246 2252 2254 2256 2257\n",
      " 2260 2262 2265 2276 2279 2284 2285 2294 2296 2302 2307 2314 2320 2340\n",
      " 2346 2350 2351 2353 2368 2372 2373 2374 2384 2387 2390 2397 2399 2400\n",
      " 2401 2403 2404 2409 2410 2417 2419 2422 2429 2430 2436 2437 2445 2449\n",
      " 2454 2457 2466 2467 2479 2484 2488 2490 2491 2493 2514 2516 2520 2522\n",
      " 2526 2527 2535 2545 2546 2547 2548 2549 2551 2555 2557 2566 2569 2574\n",
      " 2592 2593 2596 2602 2603 2606 2608 2612 2619 2636 2640 2644 2648 2649\n",
      " 2651 2654 2658 2660 2666 2668 2686 2689 2692 2696 2697 2703 2705 2716\n",
      " 2759 2761 2769 2770 2774 2775 2778 2782 2785 2786 2788 2789 2802 2804\n",
      " 2818 2824 2827 2832 2834 2837 2838 2839 2841 2846 2850 2857 2861 2862\n",
      " 2863 2866 2868 2872 2875 2879 2888 2893 2894 2901 2904 2906 2913 2917\n",
      " 2924 2925 2932 2933 2938 2952 2956 2958 2960 2961 2962 2963 2987 2989\n",
      " 3000 3002 3007 3011 3013 3019 3021 3022 3024 3026 3030 3031 3032 3033\n",
      " 3036 3042 3055 3058 3059 3063 3069 3093 3102 3115 3117 3118 3124 3129\n",
      " 3142 3163 3165 3168 3172 3177 3178 3185 3192 3208 3218 3219 3233 3240\n",
      " 3246 3247 3249 3252 3259 3267 3269 3273 3278 3279 3282 3305 3313 3319\n",
      " 3323 3327 3329 3333 3345 3347 3349 3353 3380 3390 3395 3396 3401 3407\n",
      " 3421 3432 3433 3443 3444 3449 3452 3456 3465 3471 3478 3480 3485 3494\n",
      " 3496 3505 3508 3509 3522 3525 3528 3531 3537 3539 3547 3560 3568 3574\n",
      " 3575 3588 3589 3596 3604 3605 3607 3609 3611 3612 3614 3616 3621 3626\n",
      " 3635 3645 3646 3655 3661 3662 3676 3677 3680 3685 3686 3687 3689 3692\n",
      " 3705 3707 3711 3718 3720 3723 3726 3727 3728 3731 3732 3738 3752 3756\n",
      " 3758 3759 3763 3765 3767 3768 3771 3773 3782 3790 3797 3798 3799 3805\n",
      " 3810 3814 3816 3822 3827 3829 3836 3839 3841 3842 3845 3847 3848 3858\n",
      " 3865 3867 3869 3873 3876 3883 3885 3886 3887 3894 3897 3898 3899 3903\n",
      " 3907 3909 3911 3916 3918 3921 3922 3930 3931 3935 3943 3951 3954 3956\n",
      " 3970 3974 3981 3982 3988 3989 3990 4005 4008 4012 4022 4029 4037 4057\n",
      " 4063 4067 4075 4076 4080 4084 4093 4105 4106 4107 4111 4113 4125 4130\n",
      " 4132 4136 4140 4143 4147 4159 4169 4174 4175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators' accuracies:  [91.38, 41.08, 11.14, 51.74, 79.76, 19.64, 35.33, 80.36, 31.14, 68.38]\n",
      "Estimators' Gmeans:  [0.0, 59.29, 17.12, 17.65, 81.96, 34.72, 20.44, 0.0, 49.42, 10.19]\n",
      "Estimators' Balanced accuracies:  [50.0, 67.13, 50.12, 30.82, 82.01, 56.03, 24.36, 43.97, 61.69, 38.05]\n",
      "TRAIN: [   0    1    2 ... 4174 4175 4176] TEST: [   3    5   11   14   16   18   19   29   30   35   39   55   62   70\n",
      "   71   80   86   95  100  117  121  129  130  135  145  152  161  170\n",
      "  185  186  191  192  211  218  222  225  226  227  244  245  246  257\n",
      "  258  259  261  264  282  286  301  311  313  327  329  338  342  343\n",
      "  348  350  355  364  371  373  376  377  383  386  387  389  393  400\n",
      "  408  411  413  414  418  420  426  428  429  431  443  446  448  450\n",
      "  453  454  456  461  471  475  486  498  501  503  504  507  518  521\n",
      "  522  528  538  541  542  545  550  558  559  563  566  567  571  572\n",
      "  573  581  584  594  599  605  609  611  612  614  627  632  636  643\n",
      "  644  649  650  654  658  663  671  676  691  706  712  716  739  745\n",
      "  746  748  753  756  757  759  761  764  766  768  776  777  778  782\n",
      "  783  787  788  790  791  799  804  807  809  810  822  826  827  831\n",
      "  832  838  846  854  855  860  868  872  878  879  886  894  900  905\n",
      "  912  914  919  921  931  946  951  952  959  960  965  967  969  970\n",
      "  971  975  976  981  986  988  990  991  995  997  999 1014 1022 1023\n",
      " 1024 1026 1028 1030 1042 1047 1050 1051 1054 1055 1062 1067 1075 1079\n",
      " 1084 1086 1090 1094 1099 1100 1102 1116 1119 1127 1132 1133 1135 1141\n",
      " 1146 1161 1163 1168 1187 1200 1208 1213 1226 1232 1235 1238 1243 1250\n",
      " 1252 1254 1256 1261 1272 1282 1283 1288 1299 1307 1336 1339 1340 1344\n",
      " 1365 1369 1373 1377 1381 1383 1384 1388 1395 1399 1403 1405 1421 1424\n",
      " 1427 1432 1433 1449 1453 1454 1455 1462 1464 1468 1470 1471 1474 1486\n",
      " 1487 1491 1495 1496 1497 1501 1503 1515 1518 1523 1527 1529 1543 1552\n",
      " 1555 1559 1561 1566 1572 1575 1588 1591 1592 1598 1599 1607 1618 1621\n",
      " 1629 1630 1640 1653 1654 1656 1669 1680 1681 1685 1687 1692 1706 1710\n",
      " 1714 1726 1729 1736 1737 1751 1753 1754 1755 1759 1763 1764 1766 1767\n",
      " 1779 1782 1786 1814 1816 1833 1841 1855 1861 1864 1869 1873 1875 1887\n",
      " 1889 1894 1912 1914 1924 1937 1949 1962 1965 1968 1979 1986 1991 1996\n",
      " 2000 2001 2002 2004 2006 2009 2016 2019 2020 2021 2033 2035 2039 2050\n",
      " 2052 2053 2054 2067 2073 2086 2091 2096 2114 2122 2123 2127 2136 2137\n",
      " 2138 2140 2141 2146 2149 2156 2166 2167 2171 2172 2174 2175 2181 2185\n",
      " 2186 2192 2193 2196 2198 2206 2211 2232 2235 2237 2238 2241 2253 2258\n",
      " 2266 2267 2268 2278 2287 2290 2297 2305 2306 2310 2311 2317 2324 2329\n",
      " 2330 2334 2342 2344 2354 2355 2356 2362 2363 2365 2367 2369 2375 2382\n",
      " 2383 2385 2389 2391 2393 2421 2423 2428 2434 2441 2444 2456 2463 2473\n",
      " 2474 2475 2476 2477 2478 2482 2492 2498 2499 2500 2507 2518 2519 2521\n",
      " 2525 2530 2531 2533 2537 2538 2543 2544 2559 2564 2568 2571 2572 2575\n",
      " 2580 2581 2582 2587 2588 2599 2600 2610 2613 2616 2620 2623 2628 2634\n",
      " 2641 2645 2646 2650 2663 2665 2669 2672 2673 2677 2679 2681 2685 2708\n",
      " 2714 2723 2725 2730 2732 2735 2736 2737 2742 2744 2747 2748 2750 2758\n",
      " 2763 2773 2780 2781 2784 2794 2796 2801 2806 2812 2814 2823 2833 2835\n",
      " 2847 2851 2852 2870 2883 2889 2897 2909 2912 2915 2923 2928 2929 2936\n",
      " 2943 2949 2955 2959 2965 2968 2977 2978 2979 2986 2992 2996 3017 3025\n",
      " 3029 3040 3047 3048 3050 3051 3053 3073 3078 3083 3085 3088 3091 3092\n",
      " 3096 3101 3108 3109 3110 3112 3113 3116 3121 3123 3132 3135 3140 3143\n",
      " 3146 3150 3152 3154 3155 3159 3160 3161 3170 3175 3180 3187 3188 3190\n",
      " 3202 3207 3209 3212 3242 3261 3272 3275 3276 3284 3291 3295 3296 3298\n",
      " 3300 3304 3312 3314 3315 3316 3318 3322 3328 3334 3341 3360 3361 3362\n",
      " 3365 3366 3368 3369 3372 3375 3376 3378 3384 3389 3404 3406 3408 3412\n",
      " 3413 3419 3422 3424 3425 3427 3428 3436 3437 3448 3458 3460 3464 3466\n",
      " 3468 3479 3481 3484 3491 3492 3495 3497 3498 3499 3504 3506 3512 3518\n",
      " 3529 3533 3538 3543 3544 3546 3548 3552 3554 3556 3558 3561 3565 3566\n",
      " 3569 3572 3573 3578 3582 3592 3595 3603 3615 3620 3628 3632 3638 3641\n",
      " 3643 3647 3651 3652 3653 3664 3666 3668 3670 3673 3681 3691 3693 3694\n",
      " 3699 3700 3702 3710 3714 3722 3725 3730 3734 3735 3739 3745 3747 3750\n",
      " 3753 3760 3761 3775 3777 3779 3780 3787 3796 3801 3813 3818 3819 3828\n",
      " 3831 3832 3833 3837 3844 3846 3854 3860 3878 3879 3882 3884 3888 3889\n",
      " 3892 3896 3906 3908 3923 3926 3928 3938 3940 3945 3949 3959 3961 3965\n",
      " 3966 3972 3976 3996 3999 4001 4015 4018 4019 4025 4033 4034 4035 4046\n",
      " 4047 4050 4064 4066 4073 4078 4081 4088 4089 4091 4092 4094 4095 4098\n",
      " 4102 4112 4115 4122 4142 4156 4157 4167 4171]\n",
      "Estimators' accuracies:  [89.58, 28.14, 13.41, 77.13, 46.59, 48.02, 69.34, 42.04, 51.38, 74.37]\n",
      "Estimators' Gmeans:  [0.0, 44.37, 18.28, 71.7, 15.38, 64.25, 9.42, 59.18, 16.16, 74.52]\n",
      "Estimators' Balanced accuracies:  [50.0, 59.39, 51.67, 72.0, 28.03, 69.97, 39.21, 67.14, 30.71, 74.52]\n",
      "TRAIN: [   0    3    4 ... 4174 4175 4176] TEST: [   1    2    6   12   13   25   31   33   42   43   44   48   51   52\n",
      "   54   73   79   83   88   90   96   98   99  106  108  109  111  112\n",
      "  127  128  140  148  151  160  180  181  184  193  195  199  201  204\n",
      "  205  208  214  217  219  223  228  232  233  234  239  241  242  248\n",
      "  251  252  256  260  266  281  285  289  291  293  299  302  304  310\n",
      "  314  315  317  318  322  323  326  337  345  354  357  380  381  382\n",
      "  384  388  391  399  407  415  416  417  437  455  459  460  468  472\n",
      "  479  482  483  484  489  490  494  495  499  506  511  513  515  520\n",
      "  524  525  531  535  536  551  554  560  564  568  576  579  582  583\n",
      "  586  588  590  598  610  618  620  626  628  631  634  635  637  648\n",
      "  651  657  661  668  673  674  677  686  696  703  705  708  713  722\n",
      "  728  735  744  754  762  769  772  779  780  785  789  793  795  813\n",
      "  815  817  820  824  833  836  837  841  843  845  849  851  865  866\n",
      "  873  875  880  893  897  902  904  906  911  915  917  918  924  926\n",
      "  928  934  938  940  941  944  947  974  978  985 1012 1017 1019 1027\n",
      " 1033 1038 1039 1044 1045 1048 1049 1056 1058 1065 1070 1074 1076 1083\n",
      " 1088 1089 1091 1096 1105 1107 1121 1123 1124 1128 1129 1138 1149 1157\n",
      " 1162 1169 1180 1193 1197 1199 1201 1204 1211 1214 1216 1223 1224 1230\n",
      " 1231 1234 1239 1240 1241 1242 1251 1265 1269 1270 1273 1274 1284 1293\n",
      " 1294 1296 1301 1305 1306 1308 1312 1314 1320 1326 1330 1334 1343 1356\n",
      " 1361 1366 1367 1371 1375 1382 1385 1387 1393 1400 1406 1410 1412 1416\n",
      " 1418 1420 1422 1423 1425 1426 1428 1434 1436 1440 1441 1442 1465 1466\n",
      " 1473 1476 1480 1489 1499 1500 1504 1507 1509 1511 1516 1521 1524 1534\n",
      " 1540 1550 1551 1562 1565 1571 1574 1576 1579 1581 1586 1587 1595 1603\n",
      " 1610 1615 1624 1626 1627 1642 1645 1657 1659 1660 1665 1673 1700 1702\n",
      " 1705 1711 1715 1716 1718 1719 1720 1735 1744 1748 1752 1765 1770 1773\n",
      " 1776 1780 1785 1787 1789 1791 1798 1803 1806 1810 1813 1815 1818 1822\n",
      " 1831 1837 1839 1842 1843 1845 1846 1852 1853 1857 1859 1862 1863 1867\n",
      " 1871 1878 1879 1883 1886 1888 1893 1898 1913 1917 1919 1929 1930 1936\n",
      " 1939 1944 1960 1961 1967 1971 1972 1973 1974 1982 1987 1995 2005 2015\n",
      " 2018 2022 2030 2034 2038 2041 2042 2046 2055 2057 2059 2061 2068 2078\n",
      " 2082 2093 2102 2103 2109 2111 2119 2125 2128 2132 2133 2142 2155 2161\n",
      " 2177 2188 2194 2202 2205 2207 2209 2212 2214 2217 2219 2222 2239 2263\n",
      " 2272 2273 2291 2293 2299 2301 2308 2315 2321 2322 2325 2327 2336 2337\n",
      " 2341 2343 2349 2352 2358 2364 2366 2370 2380 2386 2388 2395 2413 2415\n",
      " 2424 2431 2438 2446 2447 2452 2459 2472 2486 2513 2532 2539 2554 2556\n",
      " 2561 2562 2573 2585 2597 2604 2609 2618 2622 2624 2625 2626 2635 2637\n",
      " 2642 2647 2659 2661 2662 2678 2682 2683 2684 2694 2695 2698 2699 2700\n",
      " 2701 2704 2713 2718 2722 2726 2727 2739 2746 2752 2753 2757 2764 2768\n",
      " 2772 2776 2777 2779 2803 2807 2808 2809 2816 2819 2822 2828 2829 2840\n",
      " 2842 2845 2849 2853 2854 2855 2856 2858 2860 2864 2867 2869 2873 2877\n",
      " 2880 2890 2891 2895 2920 2926 2930 2931 2935 2937 2941 2946 2954 2957\n",
      " 2966 2971 2972 2973 2981 2988 2995 3009 3010 3018 3023 3028 3034 3038\n",
      " 3045 3046 3057 3070 3075 3079 3080 3081 3084 3095 3097 3107 3114 3120\n",
      " 3127 3133 3136 3141 3144 3151 3153 3166 3171 3193 3195 3196 3198 3201\n",
      " 3204 3216 3224 3225 3226 3227 3231 3232 3238 3239 3244 3245 3248 3253\n",
      " 3258 3262 3263 3280 3286 3287 3290 3302 3307 3309 3330 3331 3332 3335\n",
      " 3338 3339 3340 3348 3352 3354 3363 3370 3377 3381 3382 3383 3385 3391\n",
      " 3393 3394 3398 3400 3402 3409 3431 3438 3439 3442 3453 3454 3461 3463\n",
      " 3474 3475 3476 3488 3489 3490 3500 3501 3503 3514 3516 3517 3519 3523\n",
      " 3527 3532 3534 3557 3559 3570 3571 3577 3581 3583 3590 3597 3599 3600\n",
      " 3601 3602 3624 3627 3631 3642 3648 3649 3657 3671 3672 3675 3678 3682\n",
      " 3690 3697 3698 3701 3703 3704 3706 3709 3717 3719 3724 3740 3741 3742\n",
      " 3743 3754 3755 3757 3762 3764 3766 3770 3776 3783 3784 3785 3795 3800\n",
      " 3804 3806 3820 3821 3823 3824 3826 3835 3843 3857 3859 3861 3863 3868\n",
      " 3872 3875 3881 3900 3901 3904 3913 3914 3917 3925 3927 3933 3937 3958\n",
      " 3960 3962 3963 3964 3968 3977 3978 3979 3980 3983 3985 3991 4000 4002\n",
      " 4004 4006 4009 4016 4021 4023 4026 4039 4048 4049 4051 4055 4056 4059\n",
      " 4061 4070 4071 4079 4096 4097 4099 4101 4117 4118 4119 4123 4128 4131\n",
      " 4133 4138 4139 4145 4160 4161 4164 4168 4173]\n",
      "Estimators' accuracies:  [91.26, 50.9, 12.1, 49.82, 77.13, 21.92, 37.6, 18.92, 88.98, 28.86]\n",
      "Estimators' Gmeans:  [0.0, 67.22, 19.17, 19.22, 78.77, 37.9, 19.66, 33.4, 0.0, 46.95]\n",
      "Estimators' Balanced accuracies:  [50.0, 71.86, 51.84, 30.39, 78.8, 56.6, 24.94, 55.58, 48.75, 61.02]\n",
      "[90.07177033492823, 90.66985645933015, 91.37724550898204, 89.46107784431138, 91.1377245508982]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[49.93368700265252, 49.93412384716733, 50.0, 49.9331550802139, 49.93438320209974]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "accGlobal = []\n",
    "gmeanGlobal = []\n",
    "baccGlobal = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase)\n",
    "#     adaboost = SMOTEBoost(n_estimators=10,k_neighbors = 5)\n",
    "#     adaboost = RUSBoostClassifier()\n",
    "    # Entrenamiento del clasificador creado\n",
    "    # <RELLENAR>\n",
    "    adaboost.fit(X_train,y_train)\n",
    "    # Lista para almacenar el accuracy de cada clasificador base\n",
    "    listaAcc = []\n",
    "    listaGmean = []\n",
    "    listaBAcc = []\n",
    "    # Por cada clasificador base\n",
    "    for i in range(len(adaboost.estimators_)):\n",
    "        # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "        # Redondear a dos decimales\n",
    "        y_pred = adaboost.estimators_[i].predict(X_test)\n",
    "        gmean = round(geometric_mean_score(y_test, y_pred)*100,2)\n",
    "        bAcc = round(balanced_accuracy_score(y_test,y_pred)*100,2)\n",
    "        acc = round(adaboost.estimators_[i].score(X_test,y_test)*100,2)\n",
    "        # Se añade a la lista de accuracies\n",
    "        listaAcc.append(acc)\n",
    "        listaGmean.append(gmean)\n",
    "        listaBAcc.append(bAcc)\n",
    "        # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "        #titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "        # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "        #mostrar(X_test,y_test,clasificador = adaboost.estimators_[i],title=titulo)\n",
    "    print(\"Estimators' accuracies: \",listaAcc)\n",
    "    print(\"Estimators' Gmeans: \",listaGmean)\n",
    "    print(\"Estimators' Balanced accuracies: \",listaBAcc)\n",
    "\n",
    "    # Se calcula el porcentaje de acierto de AdaBoost\n",
    "    acc = adaboost.score(X_test,y_test)*100\n",
    "    accGlobal.append(acc)\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    gmean = geometric_mean_score(y_test, y_pred)*100\n",
    "    gmeanGlobal.append(gmean)\n",
    "    bAcc = balanced_accuracy_score(y_test, y_pred)*100\n",
    "    baccGlobal.append(bAcc)\n",
    "print(accGlobal)\n",
    "print(gmeanGlobal)\n",
    "print(baccGlobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stats.mean(gmeanGlobal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      1.00      0.95       947\n",
      "           1       0.50      0.01      0.02        98\n",
      "\n",
      "    accuracy                           0.91      1045\n",
      "   macro avg       0.70      0.50      0.49      1045\n",
      "weighted avg       0.87      0.91      0.86      1045\n",
      "\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.85      0.90       947\n",
      "           1       0.30      0.59      0.39        98\n",
      "\n",
      "    accuracy                           0.83      1045\n",
      "   macro avg       0.62      0.72      0.65      1045\n",
      "weighted avg       0.89      0.83      0.85      1045\n",
      "\n",
      "\n",
      "RUSBoost(n_estimators=10, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.99      0.95       947\n",
      "           1       0.29      0.05      0.09        98\n",
      "\n",
      "    accuracy                           0.90      1045\n",
      "   macro avg       0.60      0.52      0.52      1045\n",
      "weighted avg       0.85      0.90      0.87      1045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.74      0.84       947\n",
      "           1       0.25      0.83      0.38        98\n",
      "\n",
      "    accuracy                           0.75      1045\n",
      "   macro avg       0.61      0.79      0.61      1045\n",
      "weighted avg       0.91      0.75      0.80      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEEL DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intLabel(c):\n",
    "    if c==b'negative':\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51588824 12.87795     3.43036    ...  8.04468     0.\n",
      "   0.1224    ]\n",
      " [ 1.5176423  12.9777      3.53812    ...  8.52888     0.\n",
      "   0.        ]\n",
      " [ 1.52212996 14.20795     3.82099    ...  9.5726      0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.51837126 14.321       3.25974    ...  5.78508     1.62855\n",
      "   0.        ]\n",
      " [ 1.51657164 14.7998      0.         ...  8.2814      1.71045\n",
      "   0.        ]\n",
      " [ 1.51732338 14.95275     0.         ...  8.61496     1.5498\n",
      "   0.        ]]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "filepath= \"../keel_datasets/glass/glass-0-1-2-3_vs_4-5-6.dat\"\n",
    "dataset = np.loadtxt(filepath,comments='@',delimiter=\", \",\n",
    "          converters = {9: intLabel})\n",
    "X, y = dataset[:,:-1],dataset[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 163 instances for the majoritary class\n",
      "There are 51 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122 instances for the majoritary class\n",
      "There are 38 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.90      0.95        41\n",
      "           1       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.88      0.95      0.91        54\n",
      "weighted avg       0.94      0.93      0.93        54\n",
      "\n",
      "Gmean:  0.9499679070317291\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.90      0.95        41\n",
      "           1       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.88      0.95      0.91        54\n",
      "weighted avg       0.94      0.93      0.93        54\n",
      "\n",
      "Gmean:  0.9499679070317291\n",
      "\n",
      "RUSBoost(n_estimators=10, n_samples=0, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.66      0.79        41\n",
      "           1       0.48      1.00      0.65        13\n",
      "\n",
      "    accuracy                           0.74        54\n",
      "   macro avg       0.74      0.83      0.72        54\n",
      "weighted avg       0.88      0.74      0.76        54\n",
      "\n",
      "Gmean:  0.8115026712006891\n",
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.95      0.97        41\n",
      "           1       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.93      0.98      0.95        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "Gmean:  0.9753048303966929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [#AdaBoostClassifier(base_estimator=SVC(), algorithm='SAMME', n_estimators=numClasificadoresBase),\n",
    "                  AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "    print(\"Gmean: \",gmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "[ 1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      " -1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      " -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.\n",
      "  1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      " -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "filepath= \"../keel_datasets/pima/pima.dat\"\n",
    "dataset = np.loadtxt(filepath,comments='@',delimiter=\",\",\n",
    "          converters = {8: intLabel})\n",
    "X, y = dataset[:,:-1],dataset[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 instances for the majoritary class\n",
      "There are 268 instanes for the minoritary class\n",
      "There are 375 instances for the majoritary class\n",
      "There are 201 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.83      0.82       125\n",
      "           1       0.67      0.64      0.66        67\n",
      "\n",
      "    accuracy                           0.77       192\n",
      "   macro avg       0.74      0.74      0.74       192\n",
      "weighted avg       0.76      0.77      0.76       192\n",
      "\n",
      "Gmean:  0.7307326113249164\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.60      0.71       125\n",
      "           1       0.53      0.85      0.66        67\n",
      "\n",
      "    accuracy                           0.69       192\n",
      "   macro avg       0.71      0.73      0.68       192\n",
      "weighted avg       0.76      0.69      0.69       192\n",
      "\n",
      "Gmean:  0.7144562696162935\n",
      "\n",
      "RUSBoost(n_estimators=10, n_samples=0, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.80      0.83       125\n",
      "           1       0.67      0.76      0.71        67\n",
      "\n",
      "    accuracy                           0.79       192\n",
      "   macro avg       0.77      0.78      0.77       192\n",
      "weighted avg       0.80      0.79      0.79       192\n",
      "\n",
      "Gmean:  0.7803558315797974\n",
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.73      0.77       125\n",
      "           1       0.59      0.72      0.64        67\n",
      "\n",
      "    accuracy                           0.72       192\n",
      "   macro avg       0.71      0.72      0.71       192\n",
      "weighted avg       0.74      0.72      0.73       192\n",
      "\n",
      "Gmean:  0.7221857370552053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [#AdaBoostClassifier(base_estimator=SVC(), algorithm='SAMME', n_estimators=numClasificadoresBase),\n",
    "                  AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "    print(\"Gmean: \",gmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(dataset_name):\n",
    "    dataset = fetch_datasets()[dataset_name]\n",
    "    return dataset.data,dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes(y):\n",
    "    default_classes = np.unique(y)\n",
    "    print(\"Default classes of the dataset were: \",default_classes)\n",
    "    maj_class = -1\n",
    "    min_class = 1\n",
    "    if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "    #     maj_class = default_classes[0]\n",
    "    #     min_class = default_classes[1]\n",
    "        y[y==default_classes[0]] = maj_class\n",
    "        y[y==default_classes[1]] = min_class\n",
    "    else:\n",
    "    #     maj_class = default_classes[1]\n",
    "    #     min_class = default_classes[0]\n",
    "        y[y==default_classes[1]] = maj_class\n",
    "        y[y==default_classes[0]] = min_class\n",
    "\n",
    "#     print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "#     print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "    return [maj_class,min_class], maj_class, min_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, method_name, base_classifier, T):\n",
    "    if method_name=='adaboost':\n",
    "        clf = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=T)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    # Lista para almacenar el accuracy de cada clasificador base\n",
    "    listaAcc = []\n",
    "    listaGmean = []\n",
    "    listaBAcc = []\n",
    "    # Por cada clasificador base\n",
    "    for i in range(len(clf.estimators_)):\n",
    "        # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "        # Redondear a dos decimales\n",
    "        y_pred = clf.estimators_[i].predict(X_test)\n",
    "        gmean = round(geometric_mean_score(y_test, y_pred)*100,2)\n",
    "        bAcc = round(balanced_accuracy_score(y_test,y_pred)*100,2)\n",
    "        acc = round(clf.estimators_[i].score(X_test,y_test)*100,2)\n",
    "        # Se añade a la lista de accuracies\n",
    "        listaAcc.append(acc)\n",
    "        listaGmean.append(gmean)\n",
    "        listaBAcc.append(bAcc)\n",
    "        # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "        #titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "        # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "        #mostrar(X_test,y_test,clasificador = adaboost.estimators_[i],title=titulo)\n",
    "    print(\"Estimators' accuracies: \",listaAcc)\n",
    "    print(\"Estimators' Gmeans: \",listaGmean)\n",
    "    print(\"Estimators' Balanced accuracies: \",listaBAcc)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(clf, X_test, y_test):\n",
    "    # Se calcula el porcentaje de acierto de AdaBoost\n",
    "    acc = clf.score(X_test,y_test)*100\n",
    "    accGlobal.append(acc)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    gmean = geometric_mean_score(y_test, y_pred)*100\n",
    "#     gmeanGlobal.append(gmean)\n",
    "    bAcc = balanced_accuracy_score(y_test, y_pred)*100\n",
    "    baccGlobal.append(bAcc)\n",
    "    \n",
    "    return gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_method(dataset_name,method_name, T=10, k=5):\n",
    "    #fetch data from dataset\n",
    "    X, y = obtain_data(dataset_name)\n",
    "    \n",
    "    #convert, just in case, class labels to -1 (majoritary class) and 1 (minoritari class)\n",
    "    classes, maj_class, min_class = convert_classes(y)\n",
    "    \n",
    "    #number of instances of each class and IR\n",
    "    n_maj = X[y==maj_class].shape[0]\n",
    "    n_min = X[y==min_class].shape[0]\n",
    "    print(\"There are {} instances for the majoritary class\".format(n_maj))\n",
    "    print(\"There are {} instanes for the minoritary class\".format(n_min))\n",
    "    print(\"IR of the dataset: \",n_maj/n_min)\n",
    "    \n",
    "    # Llamada al constructor del clasificador \n",
    "    dtc = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "    accGlobal = []\n",
    "    gmean = []\n",
    "    baccGlobal = []\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf = train(X_train, y_train, method_name, dtc, T)\n",
    "        \n",
    "        partial_gmean = gmean_test(clf, X_test, y_test)\n",
    "        \n",
    "        gmean.append(partial_gmean)\n",
    "    \n",
    "    print(\"Rendimiento del clasificador {}: {}\".format(method_name,stats.mean(gmean)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
