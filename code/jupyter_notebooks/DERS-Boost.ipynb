{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli = fetch_datasets()['abalone']\n",
    "X, y = ecoli.data, ecoli.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# data = load_breast_cancer()\n",
    "# X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "default_classes = np.unique(y)\n",
    "print(default_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3786 instances for the majoritary class\n",
      "There are 391 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2839 instances for the majoritary class\n",
      "There are 293 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE-guided UNDERSAMPLING of the majority class instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import array\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate the initial random population from the training set\n",
    "def load_individuals(X,y,creator,n):\n",
    "    maj_samples = X[y == maj_class]\n",
    "    min_samples = X[y == min_class]\n",
    "    individuals = []\n",
    "    for i in range(n):\n",
    "        random_maj = maj_samples[random.randint(0,maj_samples.shape[0]-1)]\n",
    "        random_min = min_samples[random.randint(0,min_samples.shape[0]-1)]\n",
    "        individual = np.asarray(np.concatenate((random_maj,random_min)))\n",
    "        \n",
    "        individual = creator(individual)\n",
    "        individuals.append(individual)\n",
    "    return individuals\n",
    "\n",
    "# returns the euclidean distance between two points\n",
    "def euclidean(v1, v2):\n",
    "    return sum((p-q)**2 for p, q in zip(v1, v2)) ** .5\n",
    "\n",
    "#returns the sum of the distances from each sample in X_train to the closest center\n",
    "#we are interested in minimizing this sum of distances\n",
    "def evaluate(X,individual):\n",
    "    S = 0\n",
    "    for x in X:\n",
    "        dist = dist_to_closest_center(x,individual[:D],individual[D:])\n",
    "        S += dist\n",
    "        \n",
    "    return S,\n",
    "\n",
    "#computes the euclidean distance for both centers and returns the shortest one\n",
    "def dist_to_closest_center(x,maj_center,min_center):\n",
    "    dist_majcenter = euclidean(x,maj_center)\n",
    "    dist_mincenter = euclidean(x,min_center)\n",
    "    return min(dist_majcenter,dist_mincenter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "NDIM = D*2\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "#creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMin)\n",
    "#creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "#toolbox.register(\"attr_float\", random.uniform, -3, 3)\n",
    "#toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, NDIM)\n",
    "#toolbox.register(\"individual\", selectRandomSamplesOneForEachClass, creator.Individual)\n",
    "toolbox.register(\"population\",load_individuals, X_train, y_train, creator.Individual)\n",
    "#toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"select\", tools.selRandom, k=3)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DE_clustering(CR,F,POP_SIZE,NGEN):\n",
    "    # Differential evolution parameters\n",
    "    #CR = 0.25\n",
    "    #F = 1  \n",
    "    #MU = 300\n",
    "    #NGEN = 200    \n",
    "    \n",
    "    pop = toolbox.population(n=POP_SIZE);\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
    "    \n",
    "    # Evaluate the individuals\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, evals=len(pop), **record)\n",
    "    print(logbook.stream)\n",
    "    \n",
    "    for g in range(1, NGEN):\n",
    "        for k, agent in enumerate(pop):\n",
    "            a,b,c = toolbox.select(pop)\n",
    "            y = toolbox.clone(agent)\n",
    "            index = random.randrange(NDIM)\n",
    "            for i, value in enumerate(agent):\n",
    "                if i == index or random.random() < CR:\n",
    "                    y[i] = a[i] + F*(b[i]-c[i])\n",
    "            y.fitness.values = toolbox.evaluate(y)\n",
    "            if y.fitness > agent.fitness:\n",
    "                pop[k] = y\n",
    "            #print(pop[k].fitness)\n",
    "        hof.update(pop)\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=g, evals=len(pop), **record)\n",
    "        print(logbook.stream)\n",
    "\n",
    "    print(\"Best individual is \", hof[0], hof[0].fitness.values[0])\n",
    "    return hof[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute H clustering processes and obtain one pair of cluster centers for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tevals\tstd    \tmin   \tavg    \tmax    \n",
      "0  \t10   \t352.144\t2501.5\t3081.23\t3672.92\n",
      "1  \t10   \t231.476\t2501.5\t2959.84\t3369.8 \n",
      "2  \t10   \t223.349\t2501.5\t2933.46\t3369.8 \n",
      "3  \t10   \t246.685\t2501.5\t2887.23\t3314.08\n",
      "4  \t10   \t195.36 \t2501.5\t2842.45\t3092.24\n",
      "5  \t10   \t181.487\t2501.5\t2784.8 \t3053.21\n",
      "6  \t10   \t197.498\t2457.66\t2743.07\t2995.12\n",
      "7  \t10   \t202.089\t2434.03\t2711.84\t2995.12\n",
      "8  \t10   \t228.302\t2268.23\t2693.62\t2995.12\n",
      "9  \t10   \t206.387\t2268.23\t2666.47\t2911.9 \n",
      "10 \t10   \t189.164\t2268.23\t2614.05\t2911.9 \n",
      "11 \t10   \t149.754\t2268.23\t2558.91\t2820.2 \n",
      "12 \t10   \t141.776\t2268.23\t2548.68\t2820.2 \n",
      "13 \t10   \t102.416\t2268.23\t2488.9 \t2712.75\n",
      "14 \t10   \t120.701\t2268.23\t2459.16\t2712.75\n",
      "15 \t10   \t85.9301\t2268.23\t2415.31\t2513.01\n",
      "16 \t10   \t63.3863\t2268.23\t2365.28\t2479.25\n",
      "17 \t10   \t57.2633\t2268.23\t2341.93\t2444.12\n",
      "18 \t10   \t40.6711\t2268.23\t2314.37\t2402.89\n",
      "19 \t10   \t38.3853\t2266.31\t2303.85\t2402.89\n",
      "20 \t10   \t23.5224\t2260.89\t2289.71\t2327.57\n",
      "21 \t10   \t20.2218\t2233.43\t2270.46\t2304.43\n",
      "22 \t10   \t17.8152\t2217.97\t2257.72\t2275.78\n",
      "23 \t10   \t19.4925\t2217.97\t2250.86\t2275.78\n",
      "24 \t10   \t20.5524\t2215.91\t2243.57\t2275.78\n",
      "25 \t10   \t21.6813\t2208.25\t2242.8 \t2275.78\n",
      "26 \t10   \t24.4613\t2194.47\t2239.12\t2275.78\n",
      "27 \t10   \t19.6431\t2194.47\t2224.28\t2257.72\n",
      "28 \t10   \t19.6166\t2194.47\t2217.3 \t2257.72\n",
      "29 \t10   \t18.7439\t2190.13\t2208.64\t2257.72\n",
      "30 \t10   \t15.1032\t2190.13\t2205.07\t2245.2 \n",
      "31 \t10   \t15.1784\t2190.13\t2203.99\t2245.2 \n",
      "32 \t10   \t12.4088\t2190.13\t2199.59\t2234.59\n",
      "33 \t10   \t7.14484\t2190.13\t2197.24\t2216.72\n",
      "34 \t10   \t10.438 \t2174.57\t2192.01\t2216.72\n",
      "35 \t10   \t6.96127\t2174.57\t2186.17\t2200.48\n",
      "36 \t10   \t5.24976\t2174.26\t2183.47\t2193.28\n",
      "37 \t10   \t3.31408\t2174.26\t2179.48\t2184.84\n",
      "38 \t10   \t4.29238\t2169.24\t2176.91\t2182.57\n",
      "39 \t10   \t3.85869\t2169.24\t2175.09\t2182.57\n",
      "40 \t10   \t1.99185\t2168.97\t2172.94\t2175.98\n",
      "41 \t10   \t1.85193\t2168.97\t2172.59\t2175.23\n",
      "42 \t10   \t1.4905 \t2168.97\t2171.4 \t2174.11\n",
      "43 \t10   \t1.80033\t2168.59\t2170.55\t2174.11\n",
      "44 \t10   \t1.67287\t2168.59\t2170.33\t2173.16\n",
      "45 \t10   \t1.50194\t2168.52\t2170.02\t2173.16\n",
      "46 \t10   \t0.884518\t2168.2 \t2169.22\t2171.22\n",
      "47 \t10   \t0.44717 \t2168.2 \t2168.86\t2169.78\n",
      "48 \t10   \t0.583332\t2167.39\t2168.42\t2169.16\n",
      "49 \t10   \t0.771297\t2166.3 \t2167.69\t2168.69\n",
      "50 \t10   \t1.10045 \t2165.37\t2167.07\t2168.63\n",
      "51 \t10   \t0.847648\t2165.37\t2166.83\t2167.91\n",
      "52 \t10   \t0.672643\t2165.37\t2166.39\t2167.4 \n",
      "53 \t10   \t0.528055\t2165.37\t2166.15\t2167.22\n",
      "54 \t10   \t0.328101\t2165.37\t2165.92\t2166.55\n",
      "55 \t10   \t0.338084\t2165.37\t2165.85\t2166.54\n",
      "56 \t10   \t0.264748\t2165.32\t2165.66\t2166.26\n",
      "57 \t10   \t0.224297\t2165.25\t2165.47\t2165.94\n",
      "58 \t10   \t0.255097\t2164.73\t2165.21\t2165.56\n",
      "59 \t10   \t0.347684\t2164.52\t2164.94\t2165.51\n",
      "60 \t10   \t0.264213\t2164.52\t2164.84\t2165.23\n",
      "61 \t10   \t0.252827\t2164.45\t2164.78\t2165.15\n",
      "62 \t10   \t0.259326\t2164.39\t2164.7 \t2165.12\n",
      "63 \t10   \t0.227982\t2164.3 \t2164.59\t2164.99\n",
      "64 \t10   \t0.241718\t2163.77\t2164.16\t2164.43\n",
      "65 \t10   \t0.349508\t2163.38\t2163.99\t2164.43\n",
      "66 \t10   \t0.328145\t2163.21\t2163.79\t2164.28\n",
      "67 \t10   \t0.189034\t2163.21\t2163.53\t2163.83\n",
      "68 \t10   \t0.103944\t2163.21\t2163.42\t2163.59\n",
      "69 \t10   \t0.114086\t2163.21\t2163.41\t2163.59\n",
      "70 \t10   \t0.17046 \t2163.06\t2163.35\t2163.59\n",
      "71 \t10   \t0.197407\t2162.95\t2163.23\t2163.54\n",
      "72 \t10   \t0.132253\t2162.9 \t2163.09\t2163.37\n",
      "73 \t10   \t0.118913\t2162.9 \t2163.07\t2163.34\n",
      "74 \t10   \t0.125596\t2162.84\t2163.05\t2163.34\n",
      "75 \t10   \t0.152386\t2162.79\t2163   \t2163.34\n",
      "76 \t10   \t0.102904\t2162.79\t2162.94\t2163.13\n",
      "77 \t10   \t0.120885\t2162.7 \t2162.88\t2163.1 \n",
      "78 \t10   \t0.104446\t2162.7 \t2162.84\t2162.99\n",
      "79 \t10   \t0.0740454\t2162.67\t2162.77\t2162.93\n",
      "80 \t10   \t0.0566098\t2162.65\t2162.71\t2162.81\n",
      "81 \t10   \t0.0476932\t2162.62\t2162.67\t2162.78\n",
      "82 \t10   \t0.0594013\t2162.51\t2162.64\t2162.75\n",
      "83 \t10   \t0.0665565\t2162.4 \t2162.56\t2162.64\n",
      "84 \t10   \t0.0660539\t2162.4 \t2162.54\t2162.62\n",
      "85 \t10   \t0.0639937\t2162.4 \t2162.49\t2162.62\n",
      "86 \t10   \t0.0774897\t2162.32\t2162.44\t2162.58\n",
      "87 \t10   \t0.0541398\t2162.32\t2162.4 \t2162.47\n",
      "88 \t10   \t0.0417383\t2162.32\t2162.37\t2162.45\n",
      "89 \t10   \t0.0537231\t2162.22\t2162.34\t2162.4 \n",
      "90 \t10   \t0.0568806\t2162.22\t2162.32\t2162.4 \n",
      "91 \t10   \t0.0541025\t2162.22\t2162.29\t2162.39\n",
      "92 \t10   \t0.0442281\t2162.17\t2162.24\t2162.34\n",
      "93 \t10   \t0.0466238\t2162.15\t2162.21\t2162.32\n",
      "94 \t10   \t0.0469526\t2162.15\t2162.21\t2162.32\n",
      "95 \t10   \t0.0361608\t2162.15\t2162.2 \t2162.27\n",
      "96 \t10   \t0.0324822\t2162.12\t2162.17\t2162.22\n",
      "97 \t10   \t0.0244486\t2162.11\t2162.15\t2162.19\n",
      "98 \t10   \t0.0321121\t2162.05\t2162.12\t2162.15\n",
      "99 \t10   \t0.038212 \t2162.02\t2162.08\t2162.14\n",
      "Best individual is  Individual('d', [0.39905799901763617, 0.004962742378617336, 0.6069467111255751, 0.5904717509081439, 0.46493102620035137, 0.16693682375661212, 1.035666980627717, 0.4483960678313873, 0.24226546887058548, 0.41189211800935693, 0.0, 0.998109623984206, 0.0, 0.41806167732727106, 0.31814342036071563, 0.10229881769158432, 0.3405223501877732, 0.16381105227533707, 0.10538485112706272, 0.08063823408758368]) 2162.0164352931815\n",
      "gen\tevals\tstd    \tmin    \tavg    \tmax    \n",
      "0  \t10   \t180.003\t2288.01\t2482.76\t2953.12\n",
      "1  \t10   \t119.942\t2279.32\t2456.02\t2694.38\n",
      "2  \t10   \t81.6039\t2279.32\t2411.39\t2557.17\n",
      "3  \t10   \t99.2678\t2181.67\t2378.39\t2557.17\n",
      "4  \t10   \t103.11 \t2174.66\t2356.42\t2557.17\n",
      "5  \t10   \t112.38 \t2174.66\t2316.58\t2557.17\n",
      "6  \t10   \t53.8553\t2174.66\t2259.05\t2364.95\n",
      "7  \t10   \t53.9961\t2174.66\t2256.41\t2364.95\n",
      "8  \t10   \t37.9471\t2174.66\t2233.8 \t2291.76\n",
      "9  \t10   \t35.7228\t2174.66\t2224.57\t2291.76\n",
      "10 \t10   \t32.6423\t2174.66\t2218.58\t2291.76\n",
      "11 \t10   \t33.5831\t2174.66\t2209.14\t2291.76\n",
      "12 \t10   \t34.2877\t2171.09\t2203.91\t2288.99\n",
      "13 \t10   \t21.997 \t2171.09\t2196.67\t2245.04\n",
      "14 \t10   \t16.6082\t2165.17\t2189.1 \t2217.01\n",
      "15 \t10   \t16.4578\t2165.17\t2185.19\t2217.01\n",
      "16 \t10   \t14.3261\t2165.17\t2180.18\t2217.01\n",
      "17 \t10   \t6.9996 \t2165.17\t2173.57\t2189.59\n",
      "18 \t10   \t6.78518\t2157.63\t2169.34\t2181.46\n",
      "19 \t10   \t6.06321\t2155.43\t2167.79\t2176.1 \n",
      "20 \t10   \t5.87258\t2154.37\t2166.61\t2176.1 \n",
      "21 \t10   \t6.6713 \t2154.37\t2163.97\t2176.1 \n",
      "22 \t10   \t4.71382\t2154.37\t2161.01\t2168.45\n",
      "23 \t10   \t4.95695\t2153.57\t2159.37\t2168.45\n",
      "24 \t10   \t5.00725\t2151.79\t2157.89\t2168.45\n",
      "25 \t10   \t3.60602\t2151.15\t2156.31\t2163.92\n",
      "26 \t10   \t3.68508\t2151.02\t2155.28\t2163.92\n",
      "27 \t10   \t1.58109\t2150.77\t2152.77\t2155.9 \n",
      "28 \t10   \t1.76564\t2150.16\t2152.14\t2155.9 \n",
      "29 \t10   \t1.52483\t2150.16\t2151.76\t2155.06\n",
      "30 \t10   \t1.51862\t2150.14\t2151.44\t2155.06\n",
      "31 \t10   \t0.84166\t2148.9 \t2150.73\t2152.24\n",
      "32 \t10   \t1.03416\t2148.52\t2149.96\t2151.23\n",
      "33 \t10   \t1.08507\t2148.24\t2149.84\t2151.05\n",
      "34 \t10   \t0.92888\t2147.73\t2149.1 \t2150.97\n",
      "35 \t10   \t1.11814\t2146.49\t2148   \t2149.86\n",
      "36 \t10   \t0.877852\t2146.37\t2147.7 \t2149.12\n",
      "37 \t10   \t0.788571\t2146.1 \t2147.38\t2148.38\n",
      "38 \t10   \t0.747349\t2146.1 \t2147.17\t2148.25\n",
      "39 \t10   \t0.795382\t2145.23\t2146.58\t2148.06\n",
      "40 \t10   \t0.664283\t2145.23\t2146.3 \t2147.76\n",
      "41 \t10   \t0.50848 \t2145.23\t2146.11\t2146.95\n",
      "42 \t10   \t0.340617\t2145.23\t2145.86\t2146.28\n",
      "43 \t10   \t0.423434\t2144.88\t2145.45\t2146.21\n",
      "44 \t10   \t0.427587\t2144.78\t2145.33\t2146.21\n",
      "45 \t10   \t0.265684\t2144.78\t2145.09\t2145.71\n",
      "46 \t10   \t0.282457\t2144.77\t2145.01\t2145.71\n",
      "47 \t10   \t0.310717\t2144.42\t2144.92\t2145.58\n",
      "48 \t10   \t0.363974\t2144.41\t2144.85\t2145.58\n",
      "49 \t10   \t0.385026\t2144.28\t2144.73\t2145.58\n",
      "50 \t10   \t0.301369\t2144.28\t2144.61\t2145.41\n",
      "51 \t10   \t0.25943 \t2144.11\t2144.46\t2145.05\n",
      "52 \t10   \t0.233717\t2144.11\t2144.4 \t2144.93\n",
      "53 \t10   \t0.224374\t2144.11\t2144.33\t2144.93\n",
      "54 \t10   \t0.165641\t2143.94\t2144.23\t2144.51\n",
      "55 \t10   \t0.12266 \t2143.94\t2144.13\t2144.29\n",
      "56 \t10   \t0.109006\t2143.87\t2144.06\t2144.23\n",
      "57 \t10   \t0.116118\t2143.87\t2144.02\t2144.22\n",
      "58 \t10   \t0.106197\t2143.85\t2144   \t2144.22\n",
      "59 \t10   \t0.0616215\t2143.85\t2143.96\t2144.06\n",
      "60 \t10   \t0.0609483\t2143.83\t2143.92\t2144.03\n",
      "61 \t10   \t0.0416541\t2143.82\t2143.89\t2143.96\n",
      "62 \t10   \t0.0528763\t2143.77\t2143.87\t2143.96\n",
      "63 \t10   \t0.0442015\t2143.77\t2143.84\t2143.91\n",
      "64 \t10   \t0.0398227\t2143.72\t2143.79\t2143.85\n",
      "65 \t10   \t0.0391927\t2143.71\t2143.77\t2143.83\n",
      "66 \t10   \t0.0339982\t2143.69\t2143.73\t2143.8 \n",
      "67 \t10   \t0.0371288\t2143.68\t2143.73\t2143.8 \n",
      "68 \t10   \t0.0699055\t2143.58\t2143.69\t2143.8 \n",
      "69 \t10   \t0.0700501\t2143.58\t2143.66\t2143.8 \n",
      "70 \t10   \t0.0588945\t2143.58\t2143.65\t2143.74\n",
      "71 \t10   \t0.0387084\t2143.58\t2143.63\t2143.69\n",
      "72 \t10   \t0.0352047\t2143.55\t2143.6 \t2143.68\n",
      "73 \t10   \t0.0206243\t2143.55\t2143.58\t2143.62\n",
      "74 \t10   \t0.0279847\t2143.52\t2143.57\t2143.62\n",
      "75 \t10   \t0.0282023\t2143.52\t2143.56\t2143.62\n",
      "76 \t10   \t0.0147767\t2143.52\t2143.55\t2143.57\n",
      "77 \t10   \t0.0200873\t2143.5 \t2143.54\t2143.56\n",
      "78 \t10   \t0.0207426\t2143.5 \t2143.53\t2143.56\n",
      "79 \t10   \t0.0174701\t2143.48\t2143.52\t2143.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 \t10   \t0.0142989\t2143.48\t2143.51\t2143.53\n",
      "81 \t10   \t0.0138198\t2143.48\t2143.51\t2143.53\n",
      "82 \t10   \t0.0122204\t2143.48\t2143.5 \t2143.53\n",
      "83 \t10   \t0.0140173\t2143.48\t2143.5 \t2143.53\n",
      "84 \t10   \t0.0133222\t2143.48\t2143.49\t2143.53\n",
      "85 \t10   \t0.00864053\t2143.48\t2143.49\t2143.5 \n",
      "86 \t10   \t0.00864016\t2143.48\t2143.48\t2143.5 \n",
      "87 \t10   \t0.00773163\t2143.47\t2143.48\t2143.5 \n",
      "88 \t10   \t0.00908411\t2143.46\t2143.48\t2143.49\n",
      "89 \t10   \t0.00889026\t2143.46\t2143.48\t2143.49\n",
      "90 \t10   \t0.00747306\t2143.45\t2143.47\t2143.48\n",
      "91 \t10   \t0.00680785\t2143.45\t2143.46\t2143.48\n",
      "92 \t10   \t0.0063119 \t2143.44\t2143.46\t2143.46\n",
      "93 \t10   \t0.00701999\t2143.44\t2143.45\t2143.46\n",
      "94 \t10   \t0.00636827\t2143.43\t2143.45\t2143.46\n",
      "95 \t10   \t0.00516832\t2143.43\t2143.44\t2143.45\n",
      "96 \t10   \t0.00534634\t2143.43\t2143.44\t2143.45\n",
      "97 \t10   \t0.00491173\t2143.43\t2143.44\t2143.44\n",
      "98 \t10   \t0.00751415\t2143.42\t2143.43\t2143.44\n",
      "99 \t10   \t0.00785865\t2143.42\t2143.43\t2143.44\n",
      "Best individual is  Individual('d', [0.5, 0.0, 0.530647697840041, 0.5646170898701318, 0.44874169541332143, 0.1573430923220124, 1.0097148659798814, 0.4323484295214775, 0.21954533279819405, 0.29157073191580807, 0.0, 1.0, 0.0, 0.43093593555583604, 0.32859572601504894, 0.11027185359420727, 0.39587449942839426, 0.1772481266919934, 0.09955948775142043, 0.12441810829434446]) 2143.4161111879466\n",
      "gen\tevals\tstd    \tmin    \tavg    \tmax    \n",
      "0  \t10   \t571.811\t2501.51\t3324.09\t4182.43\n",
      "1  \t10   \t506.937\t2501.51\t3242.3 \t3883.41\n",
      "2  \t10   \t417.32 \t2501.51\t3059.17\t3742.56\n",
      "3  \t10   \t425.202\t2501.51\t3039.69\t3742.56\n",
      "4  \t10   \t349.882\t2501.51\t2957.65\t3660.74\n",
      "5  \t10   \t301.601\t2501.51\t2897.77\t3463.23\n",
      "6  \t10   \t243.181\t2501.51\t2841.99\t3265.04\n",
      "7  \t10   \t224.963\t2501.51\t2830.09\t3153.58\n",
      "8  \t10   \t197.561\t2501.51\t2795.01\t3153.58\n",
      "9  \t10   \t197.561\t2501.51\t2795.01\t3153.58\n",
      "10 \t10   \t146.054\t2501.51\t2737.48\t2970.96\n",
      "11 \t10   \t141.754\t2501.51\t2701.55\t2855.23\n",
      "12 \t10   \t121.274\t2461.01\t2614.48\t2812.52\n",
      "13 \t10   \t80.4152\t2461.01\t2559.49\t2739.76\n",
      "14 \t10   \t80.4152\t2461.01\t2559.49\t2739.76\n",
      "15 \t10   \t80.4152\t2461.01\t2559.49\t2739.76\n",
      "16 \t10   \t82.8895\t2435.84\t2540.08\t2739.76\n",
      "17 \t10   \t91.5259\t2404.13\t2529.75\t2739.76\n",
      "18 \t10   \t94.8273\t2404.13\t2516.25\t2739.76\n",
      "19 \t10   \t97.4925\t2404.13\t2512.45\t2739.76\n",
      "20 \t10   \t74.2348\t2348.18\t2472.16\t2608.09\n",
      "21 \t10   \t74.2348\t2348.18\t2472.16\t2608.09\n",
      "22 \t10   \t84.0989\t2325.25\t2459.61\t2608.09\n",
      "23 \t10   \t84.8005\t2325.25\t2455.69\t2608.09\n",
      "24 \t10   \t89.4208\t2325.25\t2448.18\t2608.09\n",
      "25 \t10   \t89.4208\t2325.25\t2448.18\t2608.09\n",
      "26 \t10   \t97.1922\t2324.51\t2439.02\t2608.09\n",
      "27 \t10   \t97.2432\t2324.51\t2438.62\t2608.09\n",
      "28 \t10   \t93.9559\t2304.86\t2427.07\t2608.09\n",
      "29 \t10   \t94.3238\t2304.86\t2426.7 \t2608.09\n",
      "30 \t10   \t95.2428\t2304.86\t2404.89\t2608.09\n",
      "31 \t10   \t96.0452\t2304.86\t2404.06\t2608.09\n",
      "32 \t10   \t99.8994\t2304.86\t2397.3 \t2608.09\n",
      "33 \t10   \t80.5007\t2275.04\t2367.85\t2501.8 \n",
      "34 \t10   \t67.6103\t2275.04\t2338.16\t2501.51\n",
      "35 \t10   \t65.3054\t2275.04\t2313.14\t2501.51\n",
      "36 \t10   \t66.9132\t2264.1 \t2306.52\t2501.51\n",
      "37 \t10   \t69.0029\t2256.83\t2300.43\t2501.51\n",
      "38 \t10   \t73.4695\t2235.9 \t2285.12\t2501.51\n",
      "39 \t10   \t74.6255\t2235.9 \t2281.86\t2501.51\n",
      "40 \t10   \t73.4862\t2235.9 \t2277.91\t2495.1 \n",
      "41 \t10   \t74.9463\t2235.59\t2271.88\t2495.1 \n",
      "42 \t10   \t75.9707\t2228.42\t2269.46\t2495.1 \n",
      "43 \t10   \t76.1058\t2228.42\t2269.1 \t2495.1 \n",
      "44 \t10   \t76.5927\t2221.27\t2267.4 \t2495.1 \n",
      "45 \t10   \t77.0316\t2221.27\t2266.11\t2495.1 \n",
      "46 \t10   \t77.4642\t2221.27\t2265.19\t2495.1 \n",
      "47 \t10   \t78.4656\t2221.27\t2260.53\t2495.1 \n",
      "48 \t10   \t78.8303\t2220.09\t2259.56\t2495.1 \n",
      "49 \t10   \t79.3943\t2220.09\t2258.03\t2495.1 \n",
      "50 \t10   \t80.4432\t2220.09\t2254.72\t2495.1 \n",
      "51 \t10   \t77.6373\t2220.09\t2251.76\t2484.32\n",
      "52 \t10   \t78.7578\t2208.82\t2248.99\t2484.32\n",
      "53 \t10   \t79.07  \t2208.82\t2247.67\t2484.32\n",
      "54 \t10   \t79.7342\t2208.82\t2245.82\t2484.32\n",
      "55 \t10   \t80.3238\t2206.53\t2244.08\t2484.32\n",
      "56 \t10   \t81.0174\t2206.53\t2241.86\t2484.32\n",
      "57 \t10   \t81.4874\t2206.53\t2240.23\t2484.32\n",
      "58 \t10   \t81.4874\t2206.53\t2240.23\t2484.32\n",
      "59 \t10   \t81.9266\t2203.61\t2239.07\t2484.32\n",
      "60 \t10   \t82.5638\t2202.18\t2237.19\t2484.32\n",
      "61 \t10   \t83.0387\t2200.91\t2235.88\t2484.32\n",
      "62 \t10   \t83.909 \t2200.91\t2232.73\t2484.32\n",
      "63 \t10   \t85.2596\t2197   \t2228.58\t2484.32\n",
      "64 \t10   \t85.3254\t2197   \t2228.38\t2484.32\n",
      "65 \t10   \t85.3704\t2197   \t2228.24\t2484.32\n",
      "66 \t10   \t85.7659\t2193.49\t2227.11\t2484.32\n",
      "67 \t10   \t86.0311\t2193.49\t2226.33\t2484.32\n",
      "68 \t10   \t70.2497\t2193.49\t2219.93\t2430.53\n",
      "69 \t10   \t47.9973\t2191.35\t2212.12\t2355.84\n",
      "70 \t10   \t46.0959\t2191.35\t2209.46\t2347.63\n",
      "71 \t10   \t17.7387\t2190.66\t2199.15\t2252.13\n",
      "72 \t10   \t17.4577\t2190.66\t2198.58\t2250.69\n",
      "73 \t10   \t1.77221\t2190.66\t2192.9 \t2196.5 \n",
      "74 \t10   \t1.74628\t2190.66\t2192.35\t2196.5 \n",
      "75 \t10   \t2.61694\t2185.67\t2191.65\t2196.5 \n",
      "76 \t10   \t2.30643\t2185.67\t2190.8 \t2194.6 \n",
      "77 \t10   \t2.06657\t2185.67\t2188.41\t2191.32\n",
      "78 \t10   \t1.48746\t2184.3 \t2187.3 \t2188.99\n",
      "79 \t10   \t1.33402\t2184.3 \t2186.86\t2188.51\n",
      "80 \t10   \t1.37998\t2182.59\t2185.2 \t2187.55\n",
      "81 \t10   \t1.15061\t2182.59\t2185   \t2186.46\n",
      "82 \t10   \t1.00435\t2182.59\t2184.69\t2186.26\n",
      "83 \t10   \t0.633141\t2182.59\t2183.84\t2184.66\n",
      "84 \t10   \t0.68383 \t2182.28\t2183.3 \t2184.49\n",
      "85 \t10   \t0.479489\t2181.9 \t2182.78\t2183.77\n",
      "86 \t10   \t0.722586\t2180.47\t2182.22\t2183.01\n",
      "87 \t10   \t0.717808\t2180.47\t2181.49\t2182.61\n",
      "88 \t10   \t0.61879 \t2180.47\t2181.26\t2182.29\n",
      "89 \t10   \t0.653526\t2180.14\t2181.12\t2182.29\n",
      "90 \t10   \t0.556413\t2180.11\t2180.8 \t2181.86\n",
      "91 \t10   \t0.527924\t2180.11\t2180.57\t2181.86\n",
      "92 \t10   \t0.316016\t2180.11\t2180.44\t2181.05\n",
      "93 \t10   \t0.216263\t2179.64\t2180.19\t2180.48\n",
      "94 \t10   \t0.222181\t2179.64\t2180.13\t2180.48\n",
      "95 \t10   \t0.192175\t2179.64\t2179.96\t2180.29\n",
      "96 \t10   \t0.179612\t2179.49\t2179.81\t2180.13\n",
      "97 \t10   \t0.183155\t2179.33\t2179.65\t2179.95\n",
      "98 \t10   \t0.295824\t2178.81\t2179.41\t2179.77\n",
      "99 \t10   \t0.213554\t2178.81\t2179.27\t2179.59\n",
      "Best individual is  Individual('d', [-0.027764754253439605, 0.9990291547146626, 0.0625, 0.4123549397051466, 0.3212932980554255, 0.10150514001637924, 0.3678465955753697, 0.1631468615722626, 0.017249614918679067, 0.09728497343588811, 0.45074369501266, 0.0013482310178289936, 0.485667735788752, 0.5682071622646205, 0.3661960426738642, 0.1330365319760699, 0.9609405518707924, 0.4370644726074424, 0.22817073360536602, 0.28366027213690015]) 2178.810364362096\n",
      "gen\tevals\tstd\tmin    \tavg    \tmax   \n",
      "0  \t10   \t557\t2448.09\t3016.15\t4081.1\n",
      "1  \t10   \t376.893\t2448.09\t2865.16\t3774.52\n",
      "2  \t10   \t355.646\t2448.09\t2853.32\t3698.17\n",
      "3  \t10   \t351.247\t2448.09\t2848.81\t3689.56\n",
      "4  \t10   \t328.942\t2448.09\t2835.85\t3583.36\n",
      "5  \t10   \t335.945\t2448.09\t2823.95\t3583.36\n",
      "6  \t10   \t335.945\t2448.09\t2823.95\t3583.36\n",
      "7  \t10   \t275.096\t2448.09\t2766.35\t3391.58\n",
      "8  \t10   \t225.842\t2448.09\t2742.53\t3153.45\n",
      "9  \t10   \t223.695\t2448.09\t2729.25\t3126.41\n",
      "10 \t10   \t165.306\t2448.09\t2646.1 \t3021.66\n",
      "11 \t10   \t165.306\t2448.09\t2646.1 \t3021.66\n",
      "12 \t10   \t167.636\t2448.09\t2641.84\t3021.66\n",
      "13 \t10   \t135.556\t2448.09\t2620.14\t2854.12\n",
      "14 \t10   \t135.571\t2359.97\t2562.41\t2854.12\n",
      "15 \t10   \t114.166\t2359.97\t2529.94\t2789.81\n",
      "16 \t10   \t113.369\t2359.97\t2526.53\t2789.81\n",
      "17 \t10   \t112.291\t2359.97\t2503.4 \t2789.81\n",
      "18 \t10   \t112.291\t2359.97\t2503.4 \t2789.81\n",
      "19 \t10   \t114.511\t2359.97\t2490.2 \t2789.81\n",
      "20 \t10   \t114.511\t2359.97\t2490.2 \t2789.81\n",
      "21 \t10   \t121.21 \t2308.5 \t2485.05\t2789.81\n",
      "22 \t10   \t133.251\t2300.81\t2466.88\t2789.81\n",
      "23 \t10   \t133.308\t2300.81\t2466.72\t2789.81\n",
      "24 \t10   \t132.014\t2300.81\t2464.08\t2789.81\n",
      "25 \t10   \t132.014\t2300.81\t2464.08\t2789.81\n",
      "26 \t10   \t135.39 \t2300.81\t2439.77\t2789.81\n",
      "27 \t10   \t136.517\t2297.26\t2438.64\t2789.81\n",
      "28 \t10   \t135.366\t2297.26\t2436.38\t2789.81\n",
      "29 \t10   \t135.43 \t2297.26\t2429.04\t2789.81\n",
      "30 \t10   \t90.7084\t2297.26\t2394.75\t2596.39\n",
      "31 \t10   \t90.7084\t2297.26\t2394.75\t2596.39\n",
      "32 \t10   \t93.2112\t2288.28\t2390.91\t2596.39\n",
      "33 \t10   \t63.0939\t2288.28\t2341.08\t2460.72\n",
      "34 \t10   \t60.0527\t2228.57\t2321.38\t2448.09\n",
      "35 \t10   \t67.1181\t2221.17\t2313.41\t2448.09\n",
      "36 \t10   \t68.7159\t2214.64\t2310.71\t2448.09\n",
      "37 \t10   \t68.7159\t2214.64\t2310.71\t2448.09\n",
      "38 \t10   \t71.6049\t2208.68\t2294.63\t2448.09\n",
      "39 \t10   \t67.1347\t2208.68\t2282.33\t2448.09\n",
      "40 \t10   \t67.9445\t2208.68\t2278.5 \t2448.09\n",
      "41 \t10   \t66.3185\t2208.68\t2271.5 \t2448.09\n",
      "42 \t10   \t66.2909\t2208.68\t2271.28\t2448.09\n",
      "43 \t10   \t66.3158\t2208.68\t2269.48\t2448.09\n",
      "44 \t10   \t66.4112\t2208.68\t2269.28\t2448.09\n",
      "45 \t10   \t74.2132\t2166.89\t2250.44\t2448.09\n",
      "46 \t10   \t76.714 \t2166.89\t2242.32\t2448.09\n",
      "47 \t10   \t76.8468\t2165   \t2235.2 \t2448.09\n",
      "48 \t10   \t80.2116\t2165   \t2216.57\t2448.09\n",
      "49 \t10   \t77.6175\t2157.68\t2211.82\t2435.16\n",
      "50 \t10   \t61.6294\t2157.68\t2200.34\t2376.97\n",
      "51 \t10   \t60.7547\t2157.68\t2195.6 \t2373.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 \t10   \t15.4307\t2157.68\t2174.8 \t2207.63\n",
      "53 \t10   \t11.7828\t2157.68\t2170.48\t2199.67\n",
      "54 \t10   \t7.49234\t2156.33\t2167.24\t2180.28\n",
      "55 \t10   \t6.68566\t2156.33\t2164.74\t2176.87\n",
      "56 \t10   \t6.32007\t2156.33\t2163.71\t2175.36\n",
      "57 \t10   \t6.95856\t2155.62\t2162.44\t2175.36\n",
      "58 \t10   \t5.89257\t2153.81\t2159.05\t2173.72\n",
      "59 \t10   \t3.56202\t2149.01\t2155.39\t2163.09\n",
      "60 \t10   \t4.36171\t2149.01\t2153.92\t2163.09\n",
      "61 \t10   \t3.6099 \t2149.01\t2153.04\t2158.96\n",
      "62 \t10   \t2.91278\t2149.01\t2152.43\t2157.28\n",
      "63 \t10   \t1.69758\t2149.01\t2151.1 \t2154.53\n",
      "64 \t10   \t1.68242\t2149.01\t2151.04\t2154.53\n",
      "65 \t10   \t1.91979\t2147.93\t2150.71\t2154.53\n",
      "66 \t10   \t1.18199\t2147.44\t2149.47\t2151.07\n",
      "67 \t10   \t1.46827\t2147.21\t2148.78\t2151.07\n",
      "68 \t10   \t1.26114\t2146.49\t2147.96\t2151.07\n",
      "69 \t10   \t0.856371\t2145.91\t2147.21\t2149.21\n",
      "70 \t10   \t0.557859\t2145.91\t2146.92\t2147.8 \n",
      "71 \t10   \t0.689825\t2145.66\t2146.71\t2147.8 \n",
      "72 \t10   \t0.664247\t2145.66\t2146.55\t2147.8 \n",
      "73 \t10   \t0.614823\t2145.44\t2146.12\t2147.09\n",
      "74 \t10   \t0.541411\t2145.37\t2145.98\t2147.09\n",
      "75 \t10   \t0.402713\t2144.74\t2145.48\t2146.17\n",
      "76 \t10   \t0.482863\t2144.04\t2145.08\t2145.78\n",
      "77 \t10   \t0.442403\t2144.04\t2145.01\t2145.78\n",
      "78 \t10   \t0.54607 \t2144   \t2144.8 \t2145.78\n",
      "79 \t10   \t0.514244\t2144   \t2144.62\t2145.65\n",
      "80 \t10   \t0.523506\t2143.99\t2144.57\t2145.65\n",
      "81 \t10   \t0.409245\t2143.85\t2144.33\t2145.3 \n",
      "82 \t10   \t0.403453\t2143.85\t2144.18\t2145.3 \n",
      "83 \t10   \t0.159232\t2143.85\t2144.03\t2144.47\n",
      "84 \t10   \t0.153796\t2143.54\t2143.8 \t2144.02\n",
      "85 \t10   \t0.148615\t2143.53\t2143.74\t2143.93\n",
      "86 \t10   \t0.129107\t2143.52\t2143.65\t2143.88\n",
      "87 \t10   \t0.22035 \t2143.07\t2143.37\t2143.79\n",
      "88 \t10   \t0.195326\t2143.07\t2143.34\t2143.64\n",
      "89 \t10   \t0.172898\t2142.85\t2143.12\t2143.51\n",
      "90 \t10   \t0.244293\t2142.66\t2143   \t2143.51\n",
      "91 \t10   \t0.23967 \t2142.66\t2142.97\t2143.48\n",
      "92 \t10   \t0.233863\t2142.66\t2142.92\t2143.48\n",
      "93 \t10   \t0.260801\t2142.53\t2142.87\t2143.48\n",
      "94 \t10   \t0.0930034\t2142.51\t2142.69\t2142.82\n",
      "95 \t10   \t0.137057 \t2142.36\t2142.62\t2142.77\n",
      "96 \t10   \t0.135432 \t2142.3 \t2142.54\t2142.75\n",
      "97 \t10   \t0.118098 \t2142.3 \t2142.52\t2142.67\n",
      "98 \t10   \t0.129499 \t2142.2 \t2142.43\t2142.67\n",
      "99 \t10   \t0.113278 \t2142.2 \t2142.37\t2142.54\n",
      "Best individual is  Individual('d', [0.44051584274894473, 0.01908389017007277, 0.576403550324244, 0.5503926904568257, 0.4631683411632823, 0.15607452652148535, 1.0092238775970377, 0.4362554773233489, 0.22316622823613375, 0.2950811997357776, 0.0, 0.9941082013123932, 0.0, 0.4200925643587355, 0.2838609631292375, 0.10558676092917554, 0.37201771111603166, 0.17176592432468119, 0.07887741141706185, 0.09648954923214026]) 2142.1962480274915\n",
      "gen\tevals\tstd    \tmin    \tavg   \tmax    \n",
      "0  \t10   \t543.444\t2288.93\t2907.7\t4125.18\n",
      "1  \t10   \t542.452\t2288.93\t2902.27\t4125.18\n",
      "2  \t10   \t397.255\t2288.93\t2823.62\t3508.24\n",
      "3  \t10   \t397.255\t2288.93\t2823.62\t3508.24\n",
      "4  \t10   \t363.944\t2288.93\t2788.57\t3508.24\n",
      "5  \t10   \t354.85 \t2288.93\t2783.89\t3461.46\n",
      "6  \t10   \t354.679\t2288.93\t2783.66\t3461.46\n",
      "7  \t10   \t354.679\t2288.93\t2783.66\t3461.46\n",
      "8  \t10   \t340.972\t2288.93\t2751.16\t3461.46\n",
      "9  \t10   \t334.651\t2288.93\t2743.07\t3461.46\n",
      "10 \t10   \t325.453\t2288.93\t2716.37\t3461.46\n",
      "11 \t10   \t253.504\t2288.93\t2677.53\t3130.6 \n",
      "12 \t10   \t253.201\t2288.93\t2676.79\t3130.6 \n",
      "13 \t10   \t180.406\t2288.93\t2545.48\t2970.93\n",
      "14 \t10   \t180.204\t2288.93\t2544.14\t2970.93\n",
      "15 \t10   \t199.311\t2288.93\t2503.75\t2970.93\n",
      "16 \t10   \t203.282\t2246.97\t2488.12\t2970.93\n",
      "17 \t10   \t206.377\t2246.97\t2483.65\t2970.93\n",
      "18 \t10   \t206.377\t2246.97\t2483.65\t2970.93\n",
      "19 \t10   \t227.176\t2218.14\t2455.74\t2970.93\n",
      "20 \t10   \t229.764\t2218.14\t2425.88\t2970.93\n",
      "21 \t10   \t234.898\t2218.14\t2413.22\t2970.93\n",
      "22 \t10   \t221.377\t2218.14\t2379.32\t2970.93\n",
      "23 \t10   \t223.533\t2218.14\t2375.28\t2970.93\n",
      "24 \t10   \t212.444\t2218.14\t2345.28\t2970.93\n",
      "25 \t10   \t213.132\t2218.14\t2343.86\t2970.93\n",
      "26 \t10   \t222.622\t2187.83\t2314.92\t2970.93\n",
      "27 \t10   \t224.897\t2187.83\t2304.24\t2970.93\n",
      "28 \t10   \t227.414\t2187.83\t2290.52\t2970.93\n",
      "29 \t10   \t228.207\t2187.83\t2288.15\t2970.93\n",
      "30 \t10   \t228.672\t2187.83\t2286.42\t2970.93\n",
      "31 \t10   \t230.326\t2179.67\t2282.3 \t2970.93\n",
      "32 \t10   \t231.235\t2179.67\t2279.54\t2970.93\n",
      "33 \t10   \t232.573\t2172.59\t2275.09\t2970.93\n",
      "34 \t10   \t233.783\t2172.59\t2271.31\t2970.93\n",
      "35 \t10   \t234.398\t2172.59\t2269.15\t2970.93\n",
      "36 \t10   \t118.276\t2172.59\t2230.29\t2582.3 \n",
      "37 \t10   \t22.7628\t2168.71\t2193.42\t2244.62\n",
      "38 \t10   \t22.724 \t2167.07\t2190.83\t2244.62\n",
      "39 \t10   \t13.3454\t2167.07\t2181.12\t2212.44\n",
      "40 \t10   \t13.712 \t2166.02\t2178.61\t2212.44\n",
      "41 \t10   \t9.87113\t2166.02\t2173.41\t2201.41\n",
      "42 \t10   \t9.83263\t2166.02\t2172.79\t2201.41\n",
      "43 \t10   \t3.106  \t2163.19\t2167.71\t2173.68\n",
      "44 \t10   \t2.57968\t2163.19\t2166.59\t2171.78\n",
      "45 \t10   \t1.89086\t2163.19\t2165.9 \t2169.37\n",
      "46 \t10   \t2.19257\t2160.43\t2164.42\t2169.23\n",
      "47 \t10   \t2.53645\t2160.43\t2163.27\t2169.23\n",
      "48 \t10   \t1.9437 \t2158.96\t2162.45\t2165.69\n",
      "49 \t10   \t1.93346\t2157.84\t2161.32\t2163.48\n",
      "50 \t10   \t2.12191\t2157.17\t2160.59\t2163.48\n",
      "51 \t10   \t1.68359\t2157.17\t2159.39\t2162.45\n",
      "52 \t10   \t1.45882\t2157.17\t2158.72\t2162.45\n",
      "53 \t10   \t1.10726\t2155.84\t2157.66\t2159.67\n",
      "54 \t10   \t1.18546\t2155.42\t2157.22\t2159.67\n",
      "55 \t10   \t1.25181\t2154.76\t2156.43\t2159.67\n",
      "56 \t10   \t1.08164\t2154.54\t2155.8 \t2158.17\n",
      "57 \t10   \t0.825437\t2153.89\t2155.17\t2156.95\n",
      "58 \t10   \t0.654569\t2153.89\t2154.92\t2155.92\n",
      "59 \t10   \t0.391315\t2153.89\t2154.55\t2155.09\n",
      "60 \t10   \t0.306895\t2153.74\t2154.18\t2154.8 \n",
      "61 \t10   \t0.30965 \t2153.39\t2153.9 \t2154.53\n",
      "62 \t10   \t0.316852\t2153.31\t2153.79\t2154.35\n",
      "63 \t10   \t0.287257\t2153.31\t2153.67\t2154.35\n",
      "64 \t10   \t0.270108\t2152.77\t2153.38\t2153.73\n",
      "65 \t10   \t0.195431\t2152.74\t2153.18\t2153.5 \n",
      "66 \t10   \t0.225215\t2152.69\t2153.04\t2153.44\n",
      "67 \t10   \t0.185608\t2152.69\t2153   \t2153.25\n",
      "68 \t10   \t0.194972\t2152.58\t2152.91\t2153.25\n",
      "69 \t10   \t0.182507\t2152.58\t2152.82\t2153.25\n",
      "70 \t10   \t0.129143\t2152.58\t2152.75\t2153.07\n",
      "71 \t10   \t0.127522\t2152.53\t2152.7 \t2153.03\n",
      "72 \t10   \t0.127993\t2152.39\t2152.64\t2152.87\n",
      "73 \t10   \t0.116155\t2152.35\t2152.52\t2152.7 \n",
      "74 \t10   \t0.102643\t2152.24\t2152.42\t2152.59\n",
      "75 \t10   \t0.0995137\t2152.22\t2152.33\t2152.49\n",
      "76 \t10   \t0.08576  \t2152.15\t2152.27\t2152.47\n",
      "77 \t10   \t0.096964 \t2152.11\t2152.21\t2152.47\n",
      "78 \t10   \t0.0703877\t2151.99\t2152.16\t2152.23\n",
      "79 \t10   \t0.0667208\t2151.99\t2152.12\t2152.22\n",
      "80 \t10   \t0.0565616\t2151.99\t2152.1 \t2152.2 \n",
      "81 \t10   \t0.0397553\t2151.99\t2152.08\t2152.16\n",
      "82 \t10   \t0.0541061\t2151.99\t2152.05\t2152.16\n",
      "83 \t10   \t0.0731265\t2151.9 \t2152   \t2152.16\n",
      "84 \t10   \t0.0476495\t2151.85\t2151.92\t2152   \n",
      "85 \t10   \t0.0428804\t2151.8 \t2151.89\t2151.93\n",
      "86 \t10   \t0.0469429\t2151.8 \t2151.87\t2151.93\n",
      "87 \t10   \t0.0653572\t2151.68\t2151.83\t2151.9 \n",
      "88 \t10   \t0.0684872\t2151.64\t2151.78\t2151.86\n",
      "89 \t10   \t0.067747 \t2151.63\t2151.75\t2151.86\n",
      "90 \t10   \t0.0605253\t2151.59\t2151.72\t2151.79\n",
      "91 \t10   \t0.046753 \t2151.59\t2151.69\t2151.75\n",
      "92 \t10   \t0.0500213\t2151.57\t2151.64\t2151.71\n",
      "93 \t10   \t0.0375226\t2151.55\t2151.62\t2151.68\n",
      "94 \t10   \t0.0422748\t2151.54\t2151.61\t2151.67\n",
      "95 \t10   \t0.0335813\t2151.54\t2151.59\t2151.63\n",
      "96 \t10   \t0.0331589\t2151.52\t2151.57\t2151.62\n",
      "97 \t10   \t0.036349 \t2151.5 \t2151.56\t2151.62\n",
      "98 \t10   \t0.0354339\t2151.47\t2151.54\t2151.6 \n",
      "99 \t10   \t0.036377 \t2151.43\t2151.52\t2151.58\n",
      "Best individual is  Individual('d', [0.37252931174962134, 0.0, 0.6286603451307582, 0.5924567772942336, 0.4537115775654884, 0.1552535491417152, 1.1222676691188163, 0.44772834049609544, 0.22713275846575187, 0.2971218805514352, 0.0, 1.0, 0.0, 0.4067664689635282, 0.32189087634768443, 0.10410089304578936, 0.371196475913204, 0.16000536491790743, 0.08061509921977227, 0.08042071313991031]) 2151.4343238869346\n",
      "gen\tevals\tstd    \tmin    \tavg   \tmax    \n",
      "0  \t10   \t514.801\t2341.56\t2991.3\t3784.22\n",
      "1  \t10   \t362.169\t2341.56\t2791.25\t3538.43\n",
      "2  \t10   \t313.944\t2341.56\t2715.47\t3401.02\n",
      "3  \t10   \t308.309\t2341.56\t2704.66\t3401.02\n",
      "4  \t10   \t312.983\t2341.56\t2689.17\t3401.02\n",
      "5  \t10   \t228.779\t2341.56\t2637.84\t3038.21\n",
      "6  \t10   \t222.666\t2341.56\t2629.4 \t3000.91\n",
      "7  \t10   \t207.504\t2341.56\t2614.45\t3000.91\n",
      "8  \t10   \t186.217\t2341.56\t2587.56\t2958.21\n",
      "9  \t10   \t180.508\t2341.56\t2568.04\t2958.21\n",
      "10 \t10   \t158.255\t2274.77\t2511.25\t2763.33\n",
      "11 \t10   \t158.255\t2274.77\t2511.25\t2763.33\n",
      "12 \t10   \t158.544\t2274.77\t2505.01\t2763.33\n",
      "13 \t10   \t158.544\t2274.77\t2505.01\t2763.33\n",
      "14 \t10   \t158.856\t2274.77\t2502.34\t2763.33\n",
      "15 \t10   \t154.887\t2274.77\t2476.4 \t2763.33\n",
      "16 \t10   \t166.07 \t2272.99\t2461.86\t2763.33\n",
      "17 \t10   \t166.07 \t2272.99\t2461.86\t2763.33\n",
      "18 \t10   \t161.73 \t2272.99\t2451.32\t2753.44\n",
      "19 \t10   \t168.22 \t2272.99\t2433.45\t2753.44\n",
      "20 \t10   \t176.203\t2249.94\t2422.1 \t2753.44\n",
      "21 \t10   \t176.203\t2249.94\t2422.1 \t2753.44\n",
      "22 \t10   \t174.538\t2249.94\t2418.19\t2753.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 \t10   \t174.538\t2249.94\t2418.19\t2753.44\n",
      "24 \t10   \t174.538\t2249.94\t2418.19\t2753.44\n",
      "25 \t10   \t151.678\t2231.36\t2362.6 \t2753.44\n",
      "26 \t10   \t156.67 \t2199.79\t2348.4 \t2753.44\n",
      "27 \t10   \t154.648\t2199.79\t2327.43\t2753.44\n",
      "28 \t10   \t157.101\t2173.87\t2307.19\t2753.44\n",
      "29 \t10   \t155.695\t2173.87\t2287.29\t2742.4 \n",
      "30 \t10   \t157.012\t2173.87\t2280.12\t2742.4 \n",
      "31 \t10   \t157.971\t2173.87\t2274.96\t2742.4 \n",
      "32 \t10   \t158.251\t2173.87\t2274.07\t2742.4 \n",
      "33 \t10   \t161.428\t2173.87\t2261.67\t2742.4 \n",
      "34 \t10   \t162.338\t2173.87\t2259.19\t2742.4 \n",
      "35 \t10   \t163.642\t2173.87\t2253.35\t2742.4 \n",
      "36 \t10   \t164.687\t2173.87\t2249.61\t2742.4 \n",
      "37 \t10   \t166.169\t2163.87\t2245.8 \t2742.4 \n",
      "38 \t10   \t100.946\t2163.87\t2218.54\t2518.23\n",
      "39 \t10   \t86.7676\t2163.87\t2211.49\t2469.28\n",
      "40 \t10   \t15.5052\t2163.87\t2183.81\t2222.55\n",
      "41 \t10   \t13.3902\t2161.95\t2178.7 \t2213.29\n",
      "42 \t10   \t11.1362\t2161.95\t2173.75\t2198.87\n",
      "43 \t10   \t8.59959\t2160.21\t2169.59\t2184.4 \n",
      "44 \t10   \t6.79236\t2157.1 \t2166.2 \t2179.6 \n",
      "45 \t10   \t5.69709\t2157.1 \t2164.92\t2174.27\n",
      "46 \t10   \t6.37106\t2154.94\t2162.93\t2174.27\n",
      "47 \t10   \t5.40619\t2153.28\t2161.34\t2171.48\n",
      "48 \t10   \t3.76563\t2153.28\t2159.3 \t2165.8 \n",
      "49 \t10   \t3.17906\t2153.28\t2157.81\t2164.51\n",
      "50 \t10   \t3.1957 \t2153.28\t2157.09\t2164.51\n",
      "51 \t10   \t3.49055\t2152.73\t2155.91\t2164.51\n",
      "52 \t10   \t3.22271\t2149.38\t2154.66\t2162.21\n",
      "53 \t10   \t2.53397\t2149.38\t2153.8 \t2159.28\n",
      "54 \t10   \t2.45798\t2149.38\t2152.39\t2158.04\n",
      "55 \t10   \t1.76807\t2149.38\t2151.65\t2155.05\n",
      "56 \t10   \t1.30787\t2149.38\t2151.09\t2153.61\n",
      "57 \t10   \t1.12163\t2148.54\t2150.26\t2152.33\n",
      "58 \t10   \t0.905698\t2148.51\t2149.76\t2151.27\n",
      "59 \t10   \t1.37886 \t2146.45\t2149.16\t2150.88\n",
      "60 \t10   \t1.37385 \t2145.98\t2148.46\t2150.88\n",
      "61 \t10   \t1.07675 \t2145.98\t2147.56\t2149.22\n",
      "62 \t10   \t1.04118 \t2145.78\t2147.22\t2149.22\n",
      "63 \t10   \t0.966968\t2145.78\t2147.06\t2149.22\n",
      "64 \t10   \t0.875214\t2145.73\t2146.75\t2148.45\n",
      "65 \t10   \t0.834215\t2145.18\t2146.4 \t2148.45\n",
      "66 \t10   \t0.844863\t2145.18\t2146.32\t2148.43\n",
      "67 \t10   \t0.520074\t2145.18\t2145.97\t2146.85\n",
      "68 \t10   \t0.450168\t2144.84\t2145.57\t2146.57\n",
      "69 \t10   \t0.518455\t2144.76\t2145.4 \t2146.51\n",
      "70 \t10   \t0.556783\t2144.28\t2145.14\t2146.35\n",
      "71 \t10   \t0.302149\t2144.28\t2144.75\t2145.37\n",
      "72 \t10   \t0.35626 \t2144.15\t2144.63\t2145.37\n",
      "73 \t10   \t0.386811\t2143.76\t2144.45\t2145.08\n",
      "74 \t10   \t0.37407 \t2143.76\t2144.36\t2145.08\n",
      "75 \t10   \t0.286835\t2143.76\t2144.28\t2144.71\n",
      "76 \t10   \t0.275399\t2143.56\t2144.11\t2144.58\n",
      "77 \t10   \t0.291157\t2143.56\t2143.99\t2144.4 \n",
      "78 \t10   \t0.388578\t2143.17\t2143.74\t2144.36\n",
      "79 \t10   \t0.428476\t2142.81\t2143.53\t2144.17\n",
      "80 \t10   \t0.42229 \t2142.81\t2143.36\t2144.17\n",
      "81 \t10   \t0.334789\t2142.81\t2143.13\t2143.82\n",
      "82 \t10   \t0.315708\t2142.78\t2143.11\t2143.82\n",
      "83 \t10   \t0.195617\t2142.62\t2142.89\t2143.27\n",
      "84 \t10   \t0.157777\t2142.62\t2142.86\t2143.12\n",
      "85 \t10   \t0.138821\t2142.62\t2142.82\t2143.09\n",
      "86 \t10   \t0.128777\t2142.62\t2142.8 \t2143.01\n",
      "87 \t10   \t0.113089\t2142.49\t2142.72\t2142.87\n",
      "88 \t10   \t0.100113\t2142.49\t2142.69\t2142.82\n",
      "89 \t10   \t0.0993782\t2142.49\t2142.64\t2142.8 \n",
      "90 \t10   \t0.0969659\t2142.46\t2142.58\t2142.8 \n",
      "91 \t10   \t0.107738 \t2142.42\t2142.55\t2142.79\n",
      "92 \t10   \t0.109304 \t2142.41\t2142.52\t2142.79\n",
      "93 \t10   \t0.0640501\t2142.41\t2142.47\t2142.63\n",
      "94 \t10   \t0.0624703\t2142.4 \t2142.46\t2142.63\n",
      "95 \t10   \t0.0672942\t2142.33\t2142.42\t2142.58\n",
      "96 \t10   \t0.0337309\t2142.33\t2142.39\t2142.46\n",
      "97 \t10   \t0.0266537\t2142.28\t2142.33\t2142.37\n",
      "98 \t10   \t0.049299 \t2142.18\t2142.3 \t2142.36\n",
      "99 \t10   \t0.0497339\t2142.18\t2142.27\t2142.33\n",
      "Best individual is  Individual('d', [0.38451450612750204, 0.0, 0.6287924299979034, 0.5522442088514241, 0.44718351541009665, 0.12497700842869926, 0.9884795213211411, 0.4328192432105937, 0.22143291852387345, 0.2342950154757902, 0.0, 1.0038937037077602, -0.0016724452617786767, 0.43646721716586073, 0.31992622346309474, 0.10989254695124234, 0.41191887884264367, 0.1806152803189927, 0.05540262382330294, 0.10944945729112099]) 2142.184096520432\n"
     ]
    }
   ],
   "source": [
    "H = 6\n",
    "clustering_centers = []\n",
    "for i in range(H):\n",
    "    clustering_centers.append(DE_clustering(0.6,0.5,10,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take majority samples and compute for each of them their cluster stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifies sample x to the class which center is closer to\n",
    "def classify(x,centers):\n",
    "    dist_majcenter = euclidean(x,centers[:len(x)])\n",
    "    dist_mincenter = euclidean(x,centers[len(x):])\n",
    "    return np.argmin([dist_majcenter,dist_mincenter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.5, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.5, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.5, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666, 0.8333333333333334, 0.8333333333333334, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "majority_samples = X_train[y_train==maj_class]\n",
    "\n",
    "cluster_stabilities = []\n",
    "for sample in majority_samples:\n",
    "    \n",
    "    S = 0\n",
    "    for clustering in clustering_centers:\n",
    "        c = classes[classify(sample,clustering)]\n",
    "        if c==maj_class:\n",
    "            S += 1\n",
    "    cluster_stabilities.append(S/H)\n",
    "\n",
    "print(cluster_stabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples in the majority class with the clustering stability value Ci higher than a given threshold alpha are non-bounday samples.\n",
    "We take alpha as 80% of the total clustering times (H???????????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(813, 10)\n",
      "(2026, 10)\n"
     ]
    }
   ],
   "source": [
    "#alpha = H*0.8 ????????????????????\n",
    "alpha = 0.8\n",
    "boundary_points = majority_samples[np.array(cluster_stabilities)<=alpha]\n",
    "non_boundary_points = majority_samples[np.array(cluster_stabilities)>alpha]\n",
    "\n",
    "print(boundary_points.shape)\n",
    "print(non_boundary_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly under-sample the non boundary points of the majority class, giving more importance to the data distribution information in the under-sample process. (RUS puro o ponderando los ejemplos en base a su Ci?????????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QU PROPORCIN DE LOS NON-BOUNDARY SELECCIONAMOS??????????'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810, 10)\n"
     ]
    }
   ],
   "source": [
    "#RUS PURO\n",
    "RUSsize = int(non_boundary_points.shape[0]*0.4)\n",
    "indices = np.random.randint(non_boundary_points.shape[0], size=RUSsize)\n",
    "nbp_us = non_boundary_points[indices]\n",
    "print(nbp_us.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810, 10)\n"
     ]
    }
   ],
   "source": [
    "#RUS ponderado por el cluster stability de cada ejemplo\n",
    "C_non_boundary = np.array(cluster_stabilities)[np.array(cluster_stabilities)>alpha]\n",
    "indices = np.random.choice(np.arange(non_boundary_points.shape[0]),size=RUSsize,\n",
    "                           p = C_non_boundary/sum(C_non_boundary))\n",
    "nbp_us = non_boundary_points[indices]\n",
    "print(nbp_us.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 10)\n"
     ]
    }
   ],
   "source": [
    "new_majorityclass_training = np.vstack((boundary_points,nbp_us))\n",
    "print(new_majorityclass_training.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen del undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento original de tamao: 3132\n",
      "De los cuales:\n",
      " \t n de ejemplos clase MAYORITARIA: 2839\n",
      " \t n de ejemplos clase MINORITARIA: 293\n",
      "CONJUNTO DE DATOS NO-BALANCEADO\n",
      "IR = 9.689419795221843\n",
      "n de ejemplos clase MAYORITARIA tras aplicar DE-guided UNDERSAMPLING: 1623\n",
      "Conjunto de entrenamiento actual de tamao: 1916\n"
     ]
    }
   ],
   "source": [
    "print(\"Conjunto de entrenamiento original de tamao: {}\".format(X_train.shape[0]))\n",
    "n_may,n_min = sum(y_train == maj_class),sum(y_train == min_class)\n",
    "print(\"De los cuales:\\n \\t n de ejemplos clase MAYORITARIA: {}\\n \\t n de ejemplos clase MINORITARIA: {}\"\n",
    "      .format(n_may,n_min))\n",
    "print(\"CONJUNTO DE DATOS NO-BALANCEADO\")\n",
    "print(\"IR = {}\".format(n_may/n_min))\n",
    "\n",
    "print(\"n de ejemplos clase MAYORITARIA tras aplicar DE-guided UNDERSAMPLING: {}\".format(new_majorityclass_training.shape[0]))\n",
    "print(\"Conjunto de entrenamiento actual de tamao: {}\".format(new_majorityclass_training.shape[0]+n_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toolbox.unregister(\"population\")\n",
    "toolbox.unregister(\"select\")\n",
    "\n",
    "toolbox.unregister(\"evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE-guided OVERSAMPLING of the minority class instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA-BOOST combined with DE-guided resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1916, 10) (1916,)\n"
     ]
    }
   ],
   "source": [
    "minority_samples = X_train[y_train==min_class]\n",
    "#prepare adaboost training set joining the undersampled majority class instances with the minority class instances\n",
    "X_US = np.vstack((new_majorityclass_training,minority_samples))\n",
    "y_US = np.hstack((np.full(new_majorityclass_training.shape[0],maj_class),\n",
    "                  np.full(minority_samples.shape[0],min_class)))\n",
    "print(X_US.shape,y_US.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply SMOTE to the original undersampled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synthetics(X,y,n_maj,n_min,maj_class,min_class): \n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    sm = SMOTE(sampling_strategy = {maj_class: n_maj,min_class: n_min*2})\n",
    "    X_after_SMOTE, y_after_SMOTE = sm.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_after_SMOTE))\n",
    "    synthetic_samples = X_after_SMOTE[y_after_SMOTE==min_class][n_min:]\n",
    "    #print('Synthetic samples: ',synthetic_samples)\n",
    "    return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual = np.random.randint(0,2,size=minority_samples.shape[0])\n",
    "# print(individual)\n",
    "# print(sum(individual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE to select the best synthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "def Gmean(clf,X_test,y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "    print(\"Gmean:\",gmean)\n",
    "    return gmean\n",
    "    \n",
    "def compute_fitness(p,X_before_SMOTE,y_before_SMOTE,maj_class,min_class,synthetic_samples, individual):\n",
    "    #print(synthetic_samples[0])\n",
    "    selected_syn = []\n",
    "    for i, value in enumerate(individual):\n",
    "        if individual[i]>0:\n",
    "            selected_syn.append(synthetic_samples[i-1])\n",
    "    #selected_syn = synthetic_samples[individual>0]\n",
    "    selected_syn = np.array(selected_syn)\n",
    "    X = np.vstack((X_before_SMOTE,selected_syn))\n",
    "    y = np.hstack((y_before_SMOTE,np.full(selected_syn.shape[0],min_class)))\n",
    "    #realizar splitTest??????????????''\n",
    "    Xtr, Xtst, ytr, ytst = train_test_split(X, y, test_size=0.3)\n",
    "    #dt = trainDT(Xtr,ytr,weights)\n",
    "    dt = trainDT(Xtr,ytr)\n",
    "    #test??????????????????????????\n",
    "    #G = Gmean(dt,X_before_SMOTE,y_before_SMOTE)\n",
    "    G = Gmean(dt,Xtst,ytst)\n",
    "    n_minority = len(individual)+sum(individual)\n",
    "    n_majority = X_before_SMOTE[y_before_SMOTE==maj_class].shape[0]\n",
    "    #f = G - abs(1-(n_minority/n_majority*p))\n",
    "    #print(\"f: \",f)\n",
    "    return G,\n",
    "\n",
    "def trainDT(X_train,y_train,w=None):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train,y_train,sample_weight=w)\n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DE_oversampling(CR,F_0,POP_SIZE,NGEN):\n",
    "    # Differential evolution parameters\n",
    "    #CR = 0.25\n",
    "    #F = 1  \n",
    "    #MU = 300\n",
    "    #NGEN = 200    \n",
    "    \n",
    "    pop = toolbox.population(n=POP_SIZE);\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = \"gen\", \"evals\", \"std\", \"min\", \"avg\", \"max\"\n",
    "    #print(pop)\n",
    "    # Evaluate the individuals\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, evals=len(pop), **record)\n",
    "    print(logbook.stream)\n",
    "    \n",
    "    for g in range(1, NGEN):\n",
    "        for k, agent in enumerate(pop):\n",
    "            a,b,c = toolbox.select(pop)\n",
    "            #we adopt a self-adaptative operator\n",
    "            #l = math.exp(1-(NGEN/(NGEN+1-g)))\n",
    "            #F = F_0*(2**l)\n",
    "            #d = toolbox.clone(agent) #donor vector\n",
    "            #sig_d = toolbox.clone(agent)\n",
    "            y = toolbox.clone(agent)\n",
    "            index = random.randrange(NDIM_DE_SMOTE)\n",
    "            for i, value in enumerate(agent):\n",
    "                #d[i] = a[i] + F*(b[i]-c[i]) #donor vector\n",
    "                #the mutated donor is mapped to binary space by a sigmoid function with displacement\n",
    "                #sig_d[i] = round(1/(1+math.exp(-(d[i]))))\n",
    "                if i == index or random.random() < CR:\n",
    "                    y[i] = a[i] + F*(b[i]-c[i])\n",
    "#                     y[i] = sig_d[i]\n",
    "            y.fitness.values = toolbox.evaluate(y)\n",
    "            if y.fitness > agent.fitness:\n",
    "                pop[k] = y\n",
    "            #print(pop[k].fitness)\n",
    "        hof.update(pop)\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=g, evals=len(pop), **record)\n",
    "        print(logbook.stream)\n",
    "\n",
    "    print(\"Best individual is \", hof[0], hof[0].fitness.values[0])\n",
    "    return hof[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDIM_DE_SMOTE = minority_samples.shape[0]\n",
    "# n_majority = new_majorityclass_training.shape[0]\n",
    "# p = 0.2\n",
    "\n",
    "# creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "# creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMax, clf=None)\n",
    "\n",
    "# toolbox = base.Toolbox()\n",
    "# toolbox.register(\"attr_int\", random.randint, 0, 1)\n",
    "# toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, NDIM_DE_SMOTE)\n",
    "# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "# toolbox.register(\"select\", tools.selRandom, k=3)\n",
    "\n",
    "# syn = np.empty(X_US.shape)\n",
    "# toolbox.register(\"evaluate\", compute_fitness, p,\n",
    "#                  X_US,y_US,maj_class,min_class,syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DERS_BOOST_classification(x,clfs,clf_weights):\n",
    "    S = 0\n",
    "    for t in range(len(clfs)):\n",
    "        y = clfs[t].predict(x)\n",
    "        S += clf_weights[t]*y\n",
    "    if S>=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DERS_Boost_train(X,y,maj_class,min_class,T=10,CR=0.6,F=0.5,POP_SIZE=10,NGEN=100):\n",
    "    N = X.shape[0]\n",
    "    minority_samples = X[y==min_class]\n",
    "    majority_samples = X[y==maj_class]\n",
    "    n_maj = majority_samples.shape[0]\n",
    "    n_min = minority_samples.shape[0]\n",
    "    \n",
    "    Xweights = np.full(N,1/N)\n",
    "    \n",
    "    #lista para almacenar el peso de cada clasificador dbil\n",
    "    clf_weights = []\n",
    "    #lista para almacenar los clasifiadores dbiles\n",
    "    clfs = []\n",
    "    \n",
    "    #configuration for the DE algorithm\n",
    "    p = 0.2\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMax, clf=None)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_int\", random.randint, 0, 1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, n_min)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"select\", tools.selRandom, k=3)\n",
    "    \n",
    "    for i in range(T):\n",
    "        \n",
    "        #generar n_min instancias sintticas de la clase minoritaria con SMOTE\n",
    "        syn = compute_synthetics(X,y,n_maj,n_min,maj_class,min_class)\n",
    "        \n",
    "        toolbox.register(\"evaluate\", compute_fitness, p,\n",
    "                     X,y,maj_class,min_class,syn)\n",
    "        toolbox.unregister(\"evaluate\")\n",
    "        #DE to select the best synthetics to train the base classifier\n",
    "        selection_mask = DE_oversampling(CR,F,POP_SIZE,NGEN)\n",
    "        selected_syn = []\n",
    "        for i, value in enumerate(selection_mask):\n",
    "            if selection_mask[i]>0:\n",
    "                selected_syn.append(syn[i])\n",
    "        selected_syn = np.array(selected_syn)\n",
    "        Xtr = np.vstack((X,selected_syn))\n",
    "        ytr = np.hstack((y,np.full(selected_syn.shape[0],min_class)))\n",
    "        \n",
    "        #asignar pesos para las instancias sintticas\n",
    "        syn_weights = np.full(selected_syn.shape[0],1/N)\n",
    "        weights = np.hstack((Xweights,syn_weights))\n",
    "        #weights = weights.astype('longdouble')\n",
    "\n",
    "        #entrenar clasificador dbil con los pesos\n",
    "        print(weights)\n",
    "        clf = trainDT(Xtr,ytr,weights)\n",
    "        Gmean(clf,X_test,y_test)\n",
    "        #calcular el error como la suma de los pesos de los ejemplos mal clasificados\n",
    "        #considerar slo los pesos de los ejemplos del cjto. original (no syn)\n",
    "        y_pred = clf.predict(X)\n",
    "        e = sum(Xweights*(y_pred!=y))\n",
    "        print(e)\n",
    "        \n",
    "        if e==1:\n",
    "            #return clfs,clf_weights\n",
    "            w_clf = -2.3\n",
    "        elif e!=0:\n",
    "            #peso del clasificador dbil\n",
    "            w_clf = 0.5*math.log((1-e)/e)\n",
    "            #actualizacin de pesos\n",
    "#             Z = 2*math.sqrt(e*(1-e))\n",
    "#             Xweights = Xweights*np.exp(-w_clf*y*y_pred)/Z\n",
    "            Xweights = Xweights*np.exp(-w_clf*y*y_pred)\n",
    "            Xweights /= sum(Xweights)\n",
    "        else:\n",
    "            w_clf = 2.3\n",
    "\n",
    "        clf_weights.append(w_clf)\n",
    "        clfs.append(clf)\n",
    "        \n",
    "    clf_weights\n",
    "    return clfs,clf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({-1: 1623, 1: 293})\n",
      "Resampled dataset shape Counter({-1: 1623, 1: 586})\n",
      "gen\tevals\tstd    \tmin    \tavg    \tmax    \n",
      "0  \t10   \t582.499\t2424.77\t3070.79\t4013.01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-210625a8a171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDERS_Boost_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_US\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_US\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaj_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNGEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-252-3d9f9dfa6f57>\u001b[0m in \u001b[0;36mDERS_Boost_train\u001b[0;34m(X, y, maj_class, min_class, T, CR, F, POP_SIZE, NGEN)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#DE to select the best synthetics to train the base classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mselection_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDE_oversampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPOP_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNGEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mselected_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-6a59902dc2be>\u001b[0m in \u001b[0;36mDE_oversampling\u001b[0;34m(CR, F_0, POP_SIZE, NGEN)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m#sig_d[i] = round(1/(1+math.exp(-(d[i]))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mCR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#                     y[i] = sig_d[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "clfs,clf_weights = DERS_Boost_train(X_US,y_US,maj_class,min_class,NGEN=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3]\n"
     ]
    }
   ],
   "source": [
    "DERS_BOOST_classification(X_test[0].reshape(1,-1),clfs,clf_weights)\n",
    "y_test[0]\n",
    "print(clf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "855\n",
      "(1045,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    c = DERS_BOOST_classification(X_test[i].reshape(1,-1),clfs,clf_weights)\n",
    "#     print(\"{} vs. {}\".format(c,y_test[i]))\n",
    "    if c!=y_test[i]:\n",
    "        print(\"error\")\n",
    "    y_pred.append(c)\n",
    "\n",
    "print(sum(y_pred==y_test))\n",
    "print(y_test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gmean: 0.5284403338836017\n",
      "293\n",
      "1623\n",
      "-0.41740070123654616\n"
     ]
    }
   ],
   "source": [
    "G=Gmean(clfs[0],X_test,y_test)\n",
    "minority_samples = X_US[y_US==min_class]\n",
    "majority_samples = X_US[y_US==maj_class]\n",
    "n_maj = majority_samples.shape[0]\n",
    "n_min = minority_samples.shape[0]\n",
    "print(n_min)\n",
    "print(n_maj)\n",
    "n_minority = n_min+n_min/2\n",
    "n_majority = n_maj\n",
    "f = G - abs(1-(n_minority/n_majority*p))\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
