{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boost (AdaB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías de AdaBoost y de árboles de decisión \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from imblearn.datasets import fetch_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = fetch_datasets()['abalone']\n",
    "X, y = abalone.data, abalone.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "satimage = fetch_datasets()['satimage']\n",
    "X, y = satimage.data, satimage.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar(X, y, clasificador=None, title=None):\n",
    "    \"\"\"\n",
    "    Esta función muestra las fronteras de decisión del clasificador ya entrenado y los ejemplos en X\n",
    "    (con el color dependiendo de y).\n",
    "    :param clasificador: Clasificador entrenado de scikit-learn\n",
    "    :param X: Matriz con los ejemplos a mostrar\n",
    "    :param y: Vector con las salidas de los ejemplos a mostrar\n",
    "    :return: Nada\n",
    "    \"\"\"\n",
    "    # Creamos los mapas de colores a utilizar\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#FFFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#FFFF00', '#0000FF'])\n",
    "\n",
    "    # Creamos la figura\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    # Primer plot a la izquierda\n",
    "    plt.subplot(111)\n",
    "\n",
    "    if clasificador is not None:\n",
    "        # Preparamos los ejemplos de entrada para poder pintar la frontera de decisión\n",
    "        # Asignamos una clase (color) a cada ejemplo de la malla en [x_min, x_max]x[y_min, y_max].\n",
    "        x_min, x_max = X[:, 0].min() * 0.9-0.05, X[:, 0].max() * 1.1\n",
    "        y_min, y_max = X[:, 1].min() * 0.9-0.05, X[:, 1].max() * 1.1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "\n",
    "        # Clasificamos los puntos\n",
    "        # <RELLENAR>\n",
    "        Z = clasificador.predict(np.hstack((xx.reshape(-1,1),yy.reshape(-1,1))))\n",
    "        # Ponemos el resultado en el formato deseado\n",
    "        # <RELLENAR>\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        # Pintamos las fronteras\n",
    "        plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.2)\n",
    "    \n",
    "    # Pintamos los puntos\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=60)\n",
    "    # Asignamos el título\n",
    "    plt.xlabel('Variable 1')\n",
    "    plt.ylabel('Variable 2')\n",
    "    if title is None:\n",
    "        plt.title(\"Ejemplos de Train\")\n",
    "    else:\n",
    "        # Establecemos el título recibido como parámetro\n",
    "        # <RELLENAR>\n",
    "        plt.title(title)    \n",
    "    \n",
    "    if clasificador is not None:\n",
    "        # Establecemos los límites\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    \n",
    "    # Mostramos la figura\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "default_classes = np.unique(y)\n",
    "print(default_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3786 instances for the majoritary class\n",
      "There are 391 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2839 instances for the majoritary class\n",
      "There are 293 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
       "                                                         max_depth=1),\n",
       "                   n_estimators=10)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se define el número de clasificadores base de AdaBoost (numClasificadoresBase)\n",
    "numClasificadoresBase = 10\n",
    "# Llamada al constructor del clasificador AdaBoost\n",
    "dtc = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase)\n",
    "# Entrenamiento del clasificador creado\n",
    "# <RELLENAR>\n",
    "adaboost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# Lista para almacenar el accuracy de cada clasificador base\n",
    "listaAcc = []\n",
    "listaGmean = []\n",
    "listaBAcc = []\n",
    "# Por cada clasificador base\n",
    "for i in range(len(adaboost.estimators_)):\n",
    "    # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "    # Redondear a dos decimales\n",
    "    y_pred = adaboost.estimators_[i].predict(X_test)\n",
    "    gmean = round(geometric_mean_score(y_test, y_pred)*100,2)\n",
    "    bAcc = round(balanced_accuracy_score(y_test,y_pred)*100,2)\n",
    "    acc = round(adaboost.estimators_[i].score(X_test,y_test)*100,2)\n",
    "    # Se añade a la lista de accuracies\n",
    "    listaAcc.append(acc)\n",
    "    listaGmean.append(gmean)\n",
    "    listaBAcc.append(bAcc)\n",
    "    # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "    #titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "    # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "    #mostrar(X_test,y_test,clasificador = adaboost.estimators_[i],title=titulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators' accuracies:  [90.62, 31.0, 12.54, 53.78, 79.23, 40.48, 49.0, 47.94, 68.9, 31.0]\n",
      "Estimators' Gmeans:  [0.0, 48.56, 18.85, 17.32, 74.62, 58.16, 19.52, 64.47, 8.8, 48.56]\n",
      "Estimators' Balanced accuracies:  [50.0, 61.02, 51.28, 31.96, 74.82, 66.24, 30.23, 69.91, 38.47, 61.02]\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimators' accuracies: \",listaAcc)\n",
    "print(\"Estimators' Gmeans: \",listaGmean)\n",
    "print(\"Estimators' Balanced accuracies: \",listaBAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar los puntos en el scatter plot se debe escoger sólo dos atriubutos\n",
    "NO SÉ HASTA QUÉ PUNTO TIENE SENTIDO Y SI ESTÁ BIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_2features = X_train[:,:2]\n",
    "# X_test_2features = X_test[:,:2]\n",
    "# # Se define el número de clasificadores base de AdaBoost (numClasificadoresBase)\n",
    "# numClasificadoresBase = 10\n",
    "# # Llamada al constructor del clasificador AdaBoost\n",
    "# dtc = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "# adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase)\n",
    "# # Entrenamiento del clasificador creado\n",
    "# # <RELLENAR>\n",
    "# adaboost.fit(X_train_2features,y_train)\n",
    "# # Lista para almacenar el accuracy de cada clasificador base\n",
    "# listaAcc = []\n",
    "# # Por cada clasificador base\n",
    "# for i in range(len(adaboost.estimators_)):\n",
    "#     # Se calcula el porcentaje de acierto del clasificador base correspondiente: adaboost.estimators_[i]\n",
    "#     # Redondear a dos decimales\n",
    "#     acc = round(adaboost.estimators_[i].score(X_test_2features,y_test)*100,2)\n",
    "#     # Se añade a la lista de accuracies\n",
    "#     listaAcc.append(acc)\n",
    "#     # Establecemos el título de la figura con el número de clasificador y su precisión en train\n",
    "#     titulo = 'Clasificador {}, accuracy: {}%'.format(i, acc)\n",
    "#     # Mostramos la figura con los datos de train y la frontera del clasificador correspondiente\n",
    "#     # <RELLENAR>\n",
    "#     mostrar(X_test_2features,y_test,clasificador = adaboost.estimators_[i],title=titulo)\n",
    "# print(listaAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.622009569378\n",
      "10.096190602031177\n",
      "50.457405771178585\n"
     ]
    }
   ],
   "source": [
    "# Se calcula el porcentaje de acierto de AdaBoost\n",
    "acc = adaboost.score(X_test,y_test)*100\n",
    "print(acc)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "gmean = geometric_mean_score(y_test, y_pred)*100\n",
    "print(gmean)\n",
    "bAcc = balanced_accuracy_score(y_test, y_pred)*100\n",
    "print(bAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE-Boost (SBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "#from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class SMOTE(object):\n",
    "    \"\"\"Implementation of Synthetic Minority Over-Sampling Technique (SMOTE).\n",
    "    SMOTE performs oversampling of the minority class by picking target \n",
    "    minority class samples and their nearest minority class neighbors and \n",
    "    generating new samples that linearly combine features of each target \n",
    "    sample with features of its selected minority class neighbors [1].\n",
    "    Parameters\n",
    "    ----------\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O. Hall, and P. Kegelmeyer. \"SMOTE:\n",
    "           Synthetic Minority Over-Sampling Technique.\" Journal of Artificial\n",
    "           Intelligence Research (JAIR), 2002.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_neighbors=5, random_state=None):\n",
    "        self.k = k_neighbors\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of new synthetic samples.\n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "\n",
    "        S = np.zeros(shape=(n_samples, self.n_features))\n",
    "        # Calculate synthetic samples.\n",
    "        for i in range(n_samples):\n",
    "            j = np.random.randint(0, self.X.shape[0])\n",
    "\n",
    "            # Find the NN for each sample.\n",
    "            # Exclude the sample itself.\n",
    "            nn = self.neigh.kneighbors(self.X[j].reshape(1, -1),\n",
    "                                       return_distance=False)[:, 1:]\n",
    "            nn_index = np.random.choice(nn[0])\n",
    "\n",
    "            dif = self.X[nn_index] - self.X[j]\n",
    "            gap = np.random.random()\n",
    "\n",
    "            S[i, :] = self.X[j, :] + gap * dif[:]\n",
    "\n",
    "        return S\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_minority_samples, n_features]\n",
    "            Holds the minority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_minority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        # Learn nearest neighbors.\n",
    "        self.neigh = NearestNeighbors(n_neighbors=self.k + 1)\n",
    "        self.neigh.fit(self.X)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class SMOTEBoost(AdaBoostClassifier):\n",
    "    \"\"\"Implementation of SMOTEBoost.\n",
    "    SMOTEBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    oversampling the minority class using SMOTE on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer.\n",
    "           \"SMOTEBoost: Improving Prediction of the Minority Class in\n",
    "           Boosting.\" European Conference on Principles of Data Mining and\n",
    "           Knowledge Discovery (PKDD), 2003.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 k_neighbors=5,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.algorithm = algorithm\n",
    "        self.smote = SMOTE(k_neighbors=k_neighbors,\n",
    "                           random_state=random_state)\n",
    "\n",
    "        super(SMOTEBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing SMOTE during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        # Check parameters.\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            X_min = X[np.where(y == self.minority_target)]\n",
    "\n",
    "            # SMOTE step.\n",
    "            if len(X_min) >= self.smote.k:\n",
    "                self.smote.fit(X_min)\n",
    "                X_syn = self.smote.sample(self.n_samples)\n",
    "                y_syn = np.full(X_syn.shape[0], fill_value=self.minority_target,\n",
    "                                dtype=np.int64)\n",
    "\n",
    "                # Normalize synthetic sample weights based on current training set.\n",
    "                sample_weight_syn = np.empty(X_syn.shape[0], dtype=np.float64)\n",
    "                sample_weight_syn[:] = 1. / X.shape[0]\n",
    "\n",
    "                # Combine the original and synthetic samples.\n",
    "                X = np.vstack((X, X_syn))\n",
    "                y = np.append(y, y_syn)\n",
    "\n",
    "                # Combine the weights.\n",
    "                sample_weight = \\\n",
    "                    np.append(sample_weight, sample_weight_syn).reshape(-1, 1)\n",
    "                sample_weight = \\\n",
    "                    np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "                # X, y, sample_weight = shuffle(X, y, sample_weight,\n",
    "                #                              random_state=random_state)\n",
    "\n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination.\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class RandomUnderSampler(object):\n",
    "    \"\"\"Implementation of random undersampling (RUS).\n",
    "    Undersample the majority class(es) by randomly picking samples with or\n",
    "    without replacement.\n",
    "    Parameters\n",
    "    ----------\n",
    "    with_replacement : bool, optional (default=True)\n",
    "        Undersample with replacement.\n",
    "    return_indices : bool, optional (default=False)\n",
    "        Whether or not to return the indices of the samples randomly selected\n",
    "        from the majority class.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, with_replacement=True, return_indices=False,\n",
    "                 random_state=None):\n",
    "        self.return_indices = return_indices\n",
    "        self.with_replacement = with_replacement\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Perform undersampling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of samples to remove.\n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_majority_samples - n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "\n",
    "        if self.n_majority_samples <= n_samples:\n",
    "            n_samples = self.n_majority_samples\n",
    "\n",
    "        idx = np.random.choice(self.n_majority_samples,\n",
    "                               size=self.n_majority_samples - n_samples,\n",
    "                               replace=self.with_replacement)\n",
    "\n",
    "        if self.return_indices:\n",
    "            return (self.X[idx], idx)\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_majority_samples, n_features]\n",
    "            Holds the majority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_majority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class RUSBoost(AdaBoostClassifier):\n",
    "    \"\"\"Implementation of RUSBoost.\n",
    "    RUSBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    undersampling the majority class using random undersampling (with or\n",
    "    without replacement) on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    min_ratio : float (default=1.0)\n",
    "        Minimum ratio of majority to minority class samples to generate.\n",
    "    with_replacement : bool, optional (default=True)\n",
    "        Undersample with replacement.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] C. Seiffert, T. M. Khoshgoftaar, J. V. Hulse, and A. Napolitano.\n",
    "           \"RUSBoost: Improving Classification Performance when Training Data\n",
    "           is Skewed\". International Conference on Pattern Recognition\n",
    "           (ICPR), 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 min_ratio=1.0,\n",
    "                 with_replacement=True,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.min_ratio = min_ratio\n",
    "        self.algorithm = algorithm\n",
    "        self.rus = RandomUnderSampler(with_replacement=with_replacement,\n",
    "                                      return_indices=True,\n",
    "                                      random_state=random_state)\n",
    "\n",
    "        super(RUSBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing random undersampling during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        # Check parameters.\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # Random undersampling step.\n",
    "            X_maj = X[np.where(y != self.minority_target)]\n",
    "            X_min = X[np.where(y == self.minority_target)]\n",
    "            self.rus.fit(X_maj)\n",
    "\n",
    "            n_maj = X_maj.shape[0]\n",
    "            n_min = X_min.shape[0]\n",
    "            if n_maj - self.n_samples < int(n_min * self.min_ratio):\n",
    "                self.n_samples = n_maj - int(n_min * self.min_ratio)\n",
    "            X_rus, X_idx = self.rus.sample(self.n_samples)\n",
    "\n",
    "            y_rus = y[np.where(y != self.minority_target)][X_idx]\n",
    "            y_min = y[np.where(y == self.minority_target)]\n",
    "\n",
    "            sample_weight_rus = \\\n",
    "                sample_weight[np.where(y != self.minority_target)][X_idx]\n",
    "            sample_weight_min = \\\n",
    "                sample_weight[np.where(y == self.minority_target)]\n",
    "\n",
    "            # Combine the minority and majority class samples.\n",
    "            X = np.vstack((X_rus, X_min))\n",
    "            y = np.append(y_rus, y_min)\n",
    "\n",
    "            # Combine the weights.\n",
    "            sample_weight = \\\n",
    "                np.append(sample_weight_rus, sample_weight_min).reshape(-1, 1)\n",
    "            sample_weight = \\\n",
    "                np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "            # X, y, sample_weight = shuffle(X, y, sample_weight,\n",
    "            #                              random_state=random_state)\n",
    "\n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination.\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      1.00      0.95       947\n",
      "           1       0.50      0.01      0.02        98\n",
      "\n",
      "    accuracy                           0.91      1045\n",
      "   macro avg       0.70      0.50      0.49      1045\n",
      "weighted avg       0.87      0.91      0.86      1045\n",
      "\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.85      0.90       947\n",
      "           1       0.30      0.59      0.39        98\n",
      "\n",
      "    accuracy                           0.83      1045\n",
      "   macro avg       0.62      0.72      0.65      1045\n",
      "weighted avg       0.89      0.83      0.85      1045\n",
      "\n",
      "\n",
      "RUSBoost(n_estimators=10, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.99      0.95       947\n",
      "           1       0.29      0.05      0.09        98\n",
      "\n",
      "    accuracy                           0.90      1045\n",
      "   macro avg       0.60      0.52      0.52      1045\n",
      "weighted avg       0.85      0.90      0.87      1045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.74      0.84       947\n",
      "           1       0.25      0.83      0.38        98\n",
      "\n",
      "    accuracy                           0.75      1045\n",
      "   macro avg       0.61      0.79      0.61      1045\n",
      "weighted avg       0.91      0.75      0.80      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEEL DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intLabel(c):\n",
    "    if c==b'negative':\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51588824 12.87795     3.43036    ...  8.04468     0.\n",
      "   0.1224    ]\n",
      " [ 1.5176423  12.9777      3.53812    ...  8.52888     0.\n",
      "   0.        ]\n",
      " [ 1.52212996 14.20795     3.82099    ...  9.5726      0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 1.51837126 14.321       3.25974    ...  5.78508     1.62855\n",
      "   0.        ]\n",
      " [ 1.51657164 14.7998      0.         ...  8.2814      1.71045\n",
      "   0.        ]\n",
      " [ 1.51732338 14.95275     0.         ...  8.61496     1.5498\n",
      "   0.        ]]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "filepath= \"../keel_datasets/glass/glass-0-1-2-3_vs_4-5-6.dat\"\n",
    "dataset = np.loadtxt(filepath,comments='@',delimiter=\", \",\n",
    "          converters = {9: intLabel})\n",
    "X, y = dataset[:,:-1],dataset[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 163 instances for the majoritary class\n",
      "There are 51 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122 instances for the majoritary class\n",
      "There are 38 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.90      0.95        41\n",
      "           1       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.88      0.95      0.91        54\n",
      "weighted avg       0.94      0.93      0.93        54\n",
      "\n",
      "Gmean:  0.9499679070317291\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.90      0.95        41\n",
      "           1       0.76      1.00      0.87        13\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.88      0.95      0.91        54\n",
      "weighted avg       0.94      0.93      0.93        54\n",
      "\n",
      "Gmean:  0.9499679070317291\n",
      "\n",
      "RUSBoost(n_estimators=10, n_samples=0, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.66      0.79        41\n",
      "           1       0.48      1.00      0.65        13\n",
      "\n",
      "    accuracy                           0.74        54\n",
      "   macro avg       0.74      0.83      0.72        54\n",
      "weighted avg       0.88      0.74      0.76        54\n",
      "\n",
      "Gmean:  0.8115026712006891\n",
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.95      0.97        41\n",
      "           1       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.93      0.98      0.95        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "Gmean:  0.9753048303966929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [#AdaBoostClassifier(base_estimator=SVC(), algorithm='SAMME', n_estimators=numClasificadoresBase),\n",
    "                  AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "    print(\"Gmean: \",gmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "[ 1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.\n",
      " -1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.\n",
      " -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      " -1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1.\n",
      " -1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1. -1. -1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
      " -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1.\n",
      "  1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      " -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1. -1.  1. -1.  1. -1. -1. -1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "filepath= \"../keel_datasets/pima/pima.dat\"\n",
    "dataset = np.loadtxt(filepath,comments='@',delimiter=\",\",\n",
    "          converters = {8: intLabel})\n",
    "X, y = dataset[:,:-1],dataset[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500 instances for the majoritary class\n",
      "There are 268 instanes for the minoritary class\n",
      "There are 375 instances for the majoritary class\n",
      "There are 201 instanes for the minoritary class\n"
     ]
    }
   ],
   "source": [
    "maj_class = -1\n",
    "min_class = 1\n",
    "if sum(y == default_classes[0]) > sum(y == default_classes[1]):\n",
    "#     maj_class = default_classes[0]\n",
    "#     min_class = default_classes[1]\n",
    "    y[y==default_classes[0]] = maj_class\n",
    "    y[y==default_classes[1]] = min_class\n",
    "else:\n",
    "#     maj_class = default_classes[1]\n",
    "#     min_class = default_classes[0]\n",
    "    y[y==default_classes[1]] = maj_class\n",
    "    y[y==default_classes[0]] = min_class\n",
    "    \n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y == min_class)))\n",
    "classes = [maj_class,min_class]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "#number of features of the dataset \n",
    "D = X_train.shape[1]\n",
    "\n",
    "print(\"There are {} instances for the majoritary class\".format(sum(y_train == maj_class)))\n",
    "print(\"There are {} instanes for the minoritary class\".format(sum(y_train == min_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
      "                                                         max_depth=1),\n",
      "                   n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.83      0.82       125\n",
      "           1       0.67      0.64      0.66        67\n",
      "\n",
      "    accuracy                           0.77       192\n",
      "   macro avg       0.74      0.74      0.74       192\n",
      "weighted avg       0.76      0.77      0.76       192\n",
      "\n",
      "Gmean:  0.7307326113249164\n",
      "\n",
      "SMOTEBoost(k_neighbors=None, n_estimators=10)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.60      0.71       125\n",
      "           1       0.53      0.85      0.66        67\n",
      "\n",
      "    accuracy                           0.69       192\n",
      "   macro avg       0.71      0.73      0.68       192\n",
      "weighted avg       0.76      0.69      0.69       192\n",
      "\n",
      "Gmean:  0.7144562696162935\n",
      "\n",
      "RUSBoost(n_estimators=10, n_samples=0, with_replacement=None)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.80      0.83       125\n",
      "           1       0.67      0.76      0.71        67\n",
      "\n",
      "    accuracy                           0.79       192\n",
      "   macro avg       0.77      0.78      0.77       192\n",
      "weighted avg       0.80      0.79      0.79       192\n",
      "\n",
      "Gmean:  0.7803558315797974\n",
      "\n",
      "RUSBoostClassifier()\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.73      0.77       125\n",
      "           1       0.59      0.72      0.64        67\n",
      "\n",
      "    accuracy                           0.72       192\n",
      "   macro avg       0.71      0.72      0.71       192\n",
      "weighted avg       0.74      0.72      0.73       192\n",
      "\n",
      "Gmean:  0.7221857370552053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/cristina/anaconda3/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "target_names = ['-1','1']\n",
    "\n",
    "for algorithm in [#AdaBoostClassifier(base_estimator=SVC(), algorithm='SAMME', n_estimators=numClasificadoresBase),\n",
    "                  AdaBoostClassifier(base_estimator=dtc, n_estimators=numClasificadoresBase),\n",
    "                  SMOTEBoost(n_estimators=10,k_neighbors = 5),\n",
    "                  RUSBoost(n_estimators=10),\n",
    "                  RUSBoostClassifier()]:\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    print()\n",
    "    print(str(algorithm))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=target_names))\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "    print(\"Gmean: \",gmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
